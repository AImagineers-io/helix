# P83: Improvement Plan for P77.md

## What's Weak / Missing

### Idempotency Gaps
- No guarantee that aggregation job is idempotent
- Missing specification for re-aggregation behavior (overwrite vs skip)
- No handling for updated raw data after aggregation runs

### Concurrency Gaps
- No locking mechanism to prevent concurrent job execution
- Missing specification for what happens if job is already running
- No distributed lock for multi-instance future

### Job Reliability Gaps
- "Survives app restart" is ambiguous - does job state persist?
- No specification for job failure recovery
- Missing monitoring/alerting for failed jobs
- No dead letter queue for permanently failed aggregations

### Time Handling Gaps
- "Midnight UTC" ambiguous - start of day or end of day?
- No handling for DST transitions (irrelevant for UTC but matters for display)
- Missing timezone awareness in backfill command
- No specification for incomplete day handling (aggregating "today")

### Resource Management Gaps
- No rate limiting for backfill command (could overwhelm database)
- Missing batch size for backfill operations
- No memory limit specification for large aggregations

## Why This Matters

Non-idempotent aggregation means re-running creates duplicates or incorrect totals. Without locking, two servers running the same job corrupt data. Job failures without alerting mean dashboards show stale data for days before anyone notices. Unbounded backfill can take down the production database.

## Proposed Improvements

### 1. Define Idempotent Aggregation

Add to Task 1 (AggregationService):
```python
async def compute_daily_aggregate(self, date: date) -> DailyAggregate:
    """
    Idempotent aggregation for a specific date.

    Behavior:
    - If aggregate exists and raw data unchanged: return existing
    - If aggregate exists and raw data changed: update in place
    - If aggregate missing: create new

    Detection: Compare hash of raw record IDs with stored hash
    """
```

Add success criteria:
- [ ] Running aggregation twice produces identical results
- [ ] Aggregate includes source_hash for change detection
- [ ] Modified raw data triggers re-aggregation on next run

### 2. Add Job Locking

Add new task:
- [ ] Aggregation job locking (2 files, ~60 LOC)
  - Tests: tests/unit/test_job_locking.py
  - Files: jobs/locking.py, jobs/__init__.py
  - Implementation: Database advisory lock (pg_advisory_lock)
  - Lock key: hash of job name + date
  - Behavior: If lock held, skip silently (another instance running)

Add success criteria:
- [ ] Two concurrent aggregation attempts for same date: only one executes
- [ ] Lock released on job completion (success or failure)
- [ ] Lock released on process crash (advisory locks auto-release)

### 3. Clarify Time Specifications

Update Task 2 (Job scheduler setup):
```markdown
Schedule: Daily at 00:05 UTC (5 minutes after midnight)
- Aggregates: Previous day (UTC day that just ended)
- Rationale: 5-minute buffer ensures all previous day's data is written

Incomplete day handling:
- Never aggregate current UTC day automatically
- Backfill command can force current day aggregation with --include-today flag
```

Add to backfill command:
```bash
# Aggregate complete days only (default)
python -m commands.backfill_aggregates --from 2024-01-01 --to 2024-01-31

# Include incomplete current day (use with caution)
python -m commands.backfill_aggregates --from 2024-01-01 --to today --include-today
```

### 4. Add Rate-Limited Backfill

Update Task 4 (Backfill command):
```python
# Backfill configuration
BATCH_SIZE = 7        # Days per batch
BATCH_DELAY_MS = 1000 # Delay between batches
MAX_CONCURRENT = 1    # Sequential by default

# Memory safety
RECORDS_PER_CHUNK = 10000  # Process raw data in chunks
```

Add success criteria:
- [ ] Backfill for 1 year completes without OOM
- [ ] Database connection pool not exhausted during backfill
- [ ] Progress logged every batch (e.g., "Aggregated 7/365 days")

### 5. Add Job Monitoring

Add new task:
- [ ] Aggregation job monitoring (2 files, ~50 LOC)
  - Tests: tests/unit/test_job_monitoring.py
  - Files: jobs/monitoring.py, jobs/__init__.py
  - Track: job_started, job_completed, job_failed events
  - Store: Last successful run timestamp in database
  - Alert condition: No successful run in 26 hours (missed one day)

Add success criteria:
- [ ] Failed aggregation logged with full error context
- [ ] Metric exposed: helix_aggregation_last_success_timestamp
- [ ] Alert fires if aggregation missing for 26 hours

### 6. Clarify "Survives Restart"

Update Task 5 (Startup scheduler integration):
```markdown
Scheduler Persistence Strategy:

Scheduler state is NOT persisted. On restart:
1. Scheduler re-registers all jobs with their cron schedules
2. Aggregation job checks what's missing and backfills automatically
3. Auto-backfill limited to last 7 days (prevent runaway on long outage)

Recovery behavior:
- App down for 1 day: Next startup aggregates missing day
- App down for 7 days: Next startup aggregates last 7 days
- App down for >7 days: Manual backfill required for older days
```

Add to AggregationService:
```python
async def auto_backfill_recent(self, days: int = 7) -> int:
    """
    Backfill any missing aggregates from last N days.
    Called on application startup.
    Returns count of days backfilled.
    """
```

Add success criterion:
- [ ] After 3-day outage and restart, previous 3 days auto-aggregated

### 7. Add Failure Recovery

Add new task:
- [ ] Aggregation failure handling (1 file, ~40 LOC)
  - Tests: tests/unit/test_aggregation_failure.py
  - Files: services/aggregation_service.py
  - On failure: Mark date as "failed" in job_runs table
  - Retry strategy: Exponential backoff, max 3 attempts
  - After 3 failures: Skip date, alert, continue to next day

Add success criteria:
- [ ] Single-day failure doesn't block subsequent days
- [ ] Failed days retried on next scheduled run (up to 3 times)
- [ ] Permanently failed days logged for manual investigation

### 8. Add Aggregate Verification

Add new task:
- [ ] Aggregate verification (1 file, ~50 LOC)
  - Tests: tests/integration/test_aggregate_verification.py
  - Files: services/aggregation_service.py
  - After aggregation: Spot-check aggregate against raw data
  - Verification: sum(raw costs) == aggregate.cost_total (within 0.01)
  - On mismatch: Log error, mark aggregate as "unverified"

Add success criterion:
- [ ] Aggregate verification runs after every aggregation
- [ ] Mismatched aggregates flagged in database

## Impact on Roadmap

- **P73** must add source_hash column to DailyAggregate model
- **P73** must add job_runs table for tracking job execution history
- **P74** analytics service should prefer verified aggregates
- **P78** dashboard should indicate unverified/stale data
- Add P89: "Aggregation Alerting Integration" - connect to PagerDuty/Slack
