# P36: Improvement Plan for P29.md

## What's Weak / Missing

- **NotSupportedError undefined**: P31 references `NotSupportedError` but P29 doesn't define it in the error types.
- **No MockProvider for testing**: Interface defined but no mock implementation for unit tests. Every downstream phase will need to create its own mocks.
- **LLMResponse missing request_id**: No correlation ID for tracing requests through the system. Debugging production issues requires linking logs.
- **No conversation history in interface**: `generate()` only takes single message. Multi-turn conversations require history parameter.
- **Registry uses class-level mutable state**: `_providers: Dict` is shared across all instances. Tests will pollute each other.
- **Config validation incomplete**: Empty string API key passes validation. No URL validation for custom endpoints.
- **No streaming support consideration**: Interface is request/response only. Streaming responses need different return type.
- **EmbeddingResponse missing dimensions**: No field to verify vector dimensions match expected (1536 for text-embedding-3-small).

---

## Why This Matters

The provider abstraction is the foundation for P30-P35. Missing error types cause import failures. No mock provider means every test file duplicates mocking logic. Missing request_id makes production debugging impossible. Class-level state causes test flakiness.

---

## Proposed Improvements

### Task 36.1: Add Missing Error Types

- [ ] Add NotSupportedError and ContentFilterError (1 file modification, ~15 LOC)
  - **Tests**: `tests/unit/test_llm_errors.py`
    - `test_not_supported_error_includes_operation()`
    - `test_content_filter_error_includes_reason()`
  - **Files**: `backend/llm/errors.py`
  - **Implementation**:
    ```python
    class NotSupportedError(LLMError):
        """Operation not supported by this provider."""
        def __init__(self, operation: str):
            self.operation = operation
            super().__init__(f"Operation not supported: {operation}")

    class ContentFilterError(LLMError):
        """Content blocked by provider's safety filter."""
        def __init__(self, reason: str = "Content flagged by safety filter"):
            self.reason = reason
            super().__init__(reason)
    ```

### Task 36.2: Add Request ID to Response Models

- [ ] Add request_id for tracing (1 file modification, ~10 LOC)
  - **Tests**: `tests/unit/test_llm_provider_interface.py`
    - `test_response_includes_request_id()`
    - `test_request_id_is_uuid()`
  - **Files**: `backend/llm/models.py`
  - **Implementation**:
    ```python
    @dataclass
    class LLMResponse:
        content: str
        model: str
        input_tokens: int
        output_tokens: int
        provider: str
        request_id: str = field(default_factory=lambda: str(uuid4()))
        cached: bool = False
        latency_ms: float = 0.0

    @dataclass
    class EmbeddingResponse:
        vectors: List[List[float]]
        model: str
        input_tokens: int
        provider: str
        dimensions: int = 0  # Set from actual vector length
        request_id: str = field(default_factory=lambda: str(uuid4()))
    ```

### Task 36.3: Create MockProvider for Testing

- [ ] Add mock provider implementation (1 file, ~60 LOC)
  - **Tests**: `tests/unit/test_mock_provider.py`
    - `test_mock_returns_configured_response()`
    - `test_mock_raises_configured_error()`
    - `test_mock_tracks_call_count()`
    - `test_mock_captures_call_arguments()`
  - **Files**: `backend/llm/providers/mock.py`
  - **Implementation**:
    ```python
    class MockProvider(LLMProvider):
        def __init__(
            self,
            response: Optional[LLMResponse] = None,
            error: Optional[LLMError] = None,
            embed_response: Optional[EmbeddingResponse] = None,
        ):
            self.response = response or LLMResponse(
                content="Mock response",
                model="mock-model",
                input_tokens=10,
                output_tokens=5,
                provider="mock",
            )
            self.error = error
            self.embed_response = embed_response
            self.calls: List[Dict] = []

        async def generate(self, system_prompt, user_message, **kwargs) -> LLMResponse:
            self.calls.append({"method": "generate", "args": (system_prompt, user_message), "kwargs": kwargs})
            if self.error:
                raise self.error
            return self.response

        async def embed(self, texts: List[str]) -> EmbeddingResponse:
            self.calls.append({"method": "embed", "texts": texts})
            if not self.embed_response:
                raise NotSupportedError("embed")
            return self.embed_response

        @property
        def supports_embeddings(self) -> bool:
            return self.embed_response is not None
    ```

### Task 36.4: Fix Registry State Isolation

- [ ] Use instance-level registry or add reset method (1 file modification, ~20 LOC)
  - **Tests**: `tests/unit/test_provider_registry.py`
    - `test_registry_reset_clears_providers()`
    - `test_registry_isolation_between_tests()`
  - **Files**: `backend/llm/registry.py`
  - **Implementation**:
    ```python
    class ProviderRegistry:
        _providers: Dict[str, Type[LLMProvider]] = {}
        _default_providers: Dict[str, Type[LLMProvider]] = {}

        @classmethod
        def register(cls, name: str, provider_class: Type[LLMProvider], is_default: bool = False):
            cls._providers[name] = provider_class
            if is_default:
                cls._default_providers[name] = provider_class

        @classmethod
        def reset(cls):
            """Reset to default providers only. Use in tests."""
            cls._providers = cls._default_providers.copy()
    ```

### Task 36.5: Add Conversation History to Interface

- [ ] Extend generate() signature for multi-turn (1 file modification, ~15 LOC)
  - **Tests**: `tests/unit/test_llm_provider_interface.py`
    - `test_generate_accepts_history_parameter()`
  - **Files**: `backend/llm/base.py`
  - **Implementation**:
    ```python
    @abstractmethod
    async def generate(
        self,
        system_prompt: str,
        user_message: str,
        context: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,  # [{"role": "user/assistant", "content": "..."}]
        max_tokens: int = 1024,
        temperature: float = 0.7,
    ) -> LLMResponse:
        """Generate text completion with optional conversation history."""
    ```

---

## Impact on Roadmap

- **P30, P31**: Must update `generate()` signature to include `history` parameter.
- **P30, P31**: Must use `request_id` from response models.
- **P32-P35**: Can use `MockProvider` instead of creating custom mocks.
- **Testing**: All LLM tests should call `ProviderRegistry.reset()` in teardown.
- **Dependencies**: Add `uuid` import to models.py.
