# P61: Validation & Duplicate Detection

## 1. Objective

**What**: Build validation pipeline and duplicate detection for parsed QA pairs before import.

**Why**: Garbage in = garbage out. Validation catches empty fields, excessive length, and malformed data. Duplicate detection prevents knowledge base pollution from near-identical questions that cause inconsistent answers and wasted embedding costs.

**Scope**:
- Included: Validation rules, text similarity calculation, duplicate detection against file and existing DB
- Excluded: Parsers (P60), batch processing (P62), API endpoints (P63)

**Dependencies**: P60 (ParsedQA schema), P59 (ImportValidationError, DuplicateDetectedError), P14 (QAPair model)

---

## 2. Implementation Tasks

### Task 61.1: Create Validation Rules

- [ ] Implement validation rules for QA pairs (1 file, ~80 LOC)
  - **Tests**: `tests/unit/test_import_validation.py`
    - `test_validates_question_not_empty()` - raises on empty/whitespace
    - `test_validates_answer_not_empty()` - raises on empty/whitespace
    - `test_validates_question_max_length()` - rejects > 10000 chars
    - `test_validates_answer_max_length()` - rejects > 50000 chars
    - `test_warns_on_short_answer()` - warning for < 20 chars
    - `test_validates_category_max_length()` - rejects > 100 chars
    - `test_validates_tags_count()` - rejects > 10 tags
    - `test_strips_whitespace_before_validation()` - "  " counts as empty
    - `test_returns_validation_result()` - valid/warnings/errors structure
  - **Files**: `backend/validators/qa_validator.py`
  - **Implementation**:
    ```python
    @dataclass
    class ValidationResult:
        valid: bool
        warnings: List[str]
        errors: List[ImportValidationError]

    class QAValidator:
        MAX_QUESTION_LENGTH = 10000
        MAX_ANSWER_LENGTH = 50000
        MIN_ANSWER_LENGTH_WARNING = 20

        def validate(self, qa: ParsedQA) -> ValidationResult:
            """Validate a single parsed QA pair."""

        def validate_batch(self, items: List[ParsedQA]) -> List[ValidationResult]:
            """Validate multiple QA pairs."""
    ```

### Task 61.2: Create Text Similarity Calculator

- [ ] Implement text similarity for duplicate detection (1 file, ~60 LOC)
  - **Tests**: `tests/unit/test_text_similarity.py`
    - `test_identical_texts_return_1()` - exact match
    - `test_different_texts_return_low()` - unrelated content
    - `test_similar_texts_return_high()` - near-duplicates
    - `test_case_insensitive()` - "Hello" == "hello"
    - `test_whitespace_normalized()` - "a  b" == "a b"
    - `test_handles_empty_string()` - returns 0
    - `test_handles_unicode()` - accented characters
  - **Files**: `backend/utils/similarity.py`
  - **Implementation**:
    ```python
    def calculate_similarity(text1: str, text2: str) -> float:
        """Calculate text similarity using sequence matching.

        Returns value between 0.0 (no match) and 1.0 (identical).
        Uses SequenceMatcher for efficiency without external deps.
        """
        # Normalize: lowercase, collapse whitespace
        # Use difflib.SequenceMatcher.ratio()
    ```

### Task 61.3: Create In-File Duplicate Detector

- [ ] Detect duplicates within the import file (1 file, ~70 LOC)
  - **Tests**: `tests/unit/test_duplicate_detection.py`
    - `test_detects_exact_duplicate()` - identical questions
    - `test_detects_near_duplicate()` - similarity > 0.9
    - `test_ignores_dissimilar()` - similarity < 0.9
    - `test_returns_first_occurrence()` - marks second as duplicate
    - `test_tracks_matched_index()` - links to original row
    - `test_configurable_threshold()` - custom similarity cutoff
    - `test_handles_large_batch()` - 1000+ items efficient
  - **Files**: `backend/validators/duplicate_detector.py`
  - **Implementation**:
    ```python
    @dataclass
    class DuplicateMatch:
        source_line: int
        matched_line: int
        similarity: float

    class InFileDuplicateDetector:
        def __init__(self, threshold: float = 0.9):
            self.threshold = threshold

        def find_duplicates(self, items: List[ParsedQA]) -> List[DuplicateMatch]:
            """Find duplicate questions within the file."""
    ```

### Task 61.4: Create Database Duplicate Detector

- [ ] Detect duplicates against existing knowledge base (1 file, ~80 LOC)
  - **Tests**: `tests/unit/test_db_duplicate_detection.py`
    - `test_detects_exact_match_in_db()` - identical question exists
    - `test_detects_near_match_in_db()` - similar question exists
    - `test_ignores_deleted_pairs()` - soft-deleted not matched
    - `test_ignores_draft_pairs()` - only active matched
    - `test_returns_existing_pair_id()` - links to DB record
    - `test_handles_empty_database()` - no errors
    - `test_batches_db_queries()` - efficient for large imports
  - **Files**: `backend/validators/db_duplicate_detector.py`
  - **Implementation**:
    ```python
    @dataclass
    class DBDuplicateMatch:
        source_line: int
        existing_id: UUID
        existing_question: str
        similarity: float

    class DBDuplicateDetector:
        def __init__(self, session: Session, threshold: float = 0.9):
            self.session = session
            self.threshold = threshold

        def find_duplicates(self, items: List[ParsedQA]) -> List[DBDuplicateMatch]:
            """Find duplicates against existing active QA pairs."""
            # Query active QA pairs, compare questions
    ```

### Task 61.5: Create Validation Pipeline

- [ ] Orchestrate validation and duplicate detection (1 file, ~60 LOC)
  - **Tests**: `tests/unit/test_validation_pipeline.py`
    - `test_runs_validation_first()` - validates before dup check
    - `test_runs_in_file_duplicates()` - checks file duplicates
    - `test_runs_db_duplicates()` - checks DB duplicates
    - `test_aggregates_all_errors()` - combined error list
    - `test_aggregates_all_warnings()` - combined warning list
    - `test_returns_valid_items()` - filtered list of good items
    - `test_marks_duplicates_as_warnings()` - not errors by default
  - **Files**: `backend/validators/validation_pipeline.py`
  - **Implementation**:
    ```python
    @dataclass
    class PipelineResult:
        valid_items: List[ParsedQA]
        validation_errors: List[ImportValidationError]
        duplicate_warnings: List[DuplicateMatch]
        warnings: List[str]

    class ValidationPipeline:
        def process(self, items: List[ParsedQA], session: Session) -> PipelineResult:
            """Run full validation and duplicate detection."""
    ```

---

## 3. Success Criteria

- [ ] Validation catches empty, too-long, and malformed data
- [ ] Short answers generate warnings, not errors
- [ ] Text similarity correctly identifies near-duplicates
- [ ] In-file duplicates detected with row numbers
- [ ] DB duplicates detected with existing pair IDs
- [ ] Pipeline aggregates all issues for reporting
- [ ] Duplicate detection efficient for 1000+ items
- [ ] All tests pass: `pytest tests/unit/test_*validation*.py tests/unit/test_*duplicate*.py tests/unit/test_*similarity*.py -v`
- [ ] Coverage â‰¥80% for validation modules
