# P27d: Chat Pipeline - RAG & LLM Processors

## 1. Objective

**What**: Build RAG retrieval and LLM generation processors—the core intelligence of the chatbot.

**Why**: RAG retrieval finds relevant QA pairs from the knowledge base using semantic search. LLM generation produces grounded responses using retrieved context. Without these, the chatbot is just a passthrough with no domain knowledge.

**Scope**:
- Included: RAG retrieval processor, LLM generation processor
- Excluded: Pipeline architecture (P27a), language (P27b), moderation/intent (P27c)

**Dependencies**: P27a (Pipeline Architecture), P13 (pgvector), P15b (QA Service), P14 (QA models)

---

## 2. Implementation Tasks

### Task 27d.1: Create Similarity Search Service

- [ ] Search QA pairs by vector similarity (1 file, ~80 LOC)
  - **Tests**: `tests/unit/test_similarity_search.py`
    - `test_finds_similar_qa_pairs()` - returns ranked results
    - `test_respects_top_k_limit()` - configurable result count
    - `test_respects_min_similarity()` - filters low scores
    - `test_filters_by_status_active()` - only active QA pairs
    - `test_returns_similarity_scores()` - score included in result
    - `test_handles_no_results()` - empty list, not error
    - `test_handles_no_embeddings()` - graceful when DB empty
  - **Files**: `backend/services/similarity_service.py`
  - **Implementation**:
    ```python
    class SimilarityService:
        def __init__(self, session: Session, embedding_service: EmbeddingService):
            self._session = session
            self._embedding_service = embedding_service

        async def search(
            self, query: str, top_k: int = 3, min_similarity: float = 0.7
        ) -> List[SimilarityResult]:
            """Find similar QA pairs using cosine similarity."""

    @dataclass
    class SimilarityResult:
        qa_pair: QAPair
        similarity_score: float
    ```

### Task 27d.2: Create RAG Retrieval Processor

- [ ] Retrieve relevant QA context for LLM (1 file, ~60 LOC)
  - **Tests**: `tests/unit/test_rag_retrieval_processor.py`
    - `test_retrieves_relevant_qa_pairs()` - populates context
    - `test_uses_translated_message()` - queries in English
    - `test_respects_config_top_k()` - config drives result count
    - `test_respects_min_similarity()` - threshold filtering
    - `test_handles_no_results()` - continues with empty context
    - `test_skips_if_moderated()` - no retrieval for flagged
    - `test_skips_for_greeting_intent()` - no retrieval needed
    - `test_processor_is_optional()` - failure doesn't halt
  - **Files**: `backend/pipeline/processors/rag_retrieval.py`
  - **Implementation**:
    ```python
    class RAGRetrievalProcessor(Processor):
        name = "rag_retrieval"
        is_optional = True

        def __init__(self, similarity_service: SimilarityService, config: PipelineConfig):
            self._similarity_service = similarity_service
            self._config = config

        async def process(self, context: PipelineContext) -> PipelineContext:
            """Retrieve relevant QA pairs for the query."""
            # Use translated_message if available, else original
            # Store results in context.retrieved_qa_pairs

        def should_skip(self, context: PipelineContext) -> bool:
            """Skip if moderated or non-question intent."""
            return context.is_moderated or context.intent in ("greeting", "farewell")
    ```

### Task 27d.3: Create LLM Prompt Builder

- [ ] Build prompts with context and history (1 file, ~70 LOC)
  - **Tests**: `tests/unit/test_llm_prompt_builder.py`
    - `test_includes_system_prompt()` - from prompt service
    - `test_includes_retrieved_context()` - QA pairs formatted
    - `test_includes_conversation_history()` - multi-turn context
    - `test_includes_user_message()` - current query
    - `test_respects_token_limits()` - truncates if needed
    - `test_handles_no_context()` - works without QA pairs
    - `test_formats_qa_pairs_clearly()` - readable format
  - **Files**: `backend/pipeline/processors/llm_prompt_builder.py`
  - **Implementation**:
    ```python
    class LLMPromptBuilder:
        def __init__(self, prompt_service: PromptService, max_context_tokens: int = 2000):
            self._prompt_service = prompt_service
            self._max_context_tokens = max_context_tokens

        def build(self, context: PipelineContext) -> List[Message]:
            """Build LLM messages from pipeline context."""
            # Format: system prompt + context + history + user message

        def _format_qa_context(self, qa_pairs: List[SimilarityResult]) -> str:
            """Format QA pairs as context block."""
    ```

### Task 27d.4: Create LLM Generation Processor

- [ ] Generate response using LLM with context (1 file, ~80 LOC)
  - **Tests**: `tests/unit/test_llm_generation_processor.py`
    - `test_generates_response()` - calls LLM, stores result
    - `test_uses_retrieved_context()` - context in prompt
    - `test_uses_conversation_history()` - multi-turn aware
    - `test_handles_no_context()` - generates without QA pairs
    - `test_handles_llm_failure()` - graceful error handling
    - `test_tracks_token_usage()` - cost tracking
    - `test_skips_if_moderated()` - no generation for flagged
    - `test_skips_if_canned_response()` - greeting already answered
  - **Files**: `backend/pipeline/processors/llm_generation.py`
  - **Implementation**:
    ```python
    class LLMGenerationProcessor(Processor):
        name = "llm_generation"
        is_optional = False  # Core functionality

        def __init__(self, llm_client: LLMClient, prompt_builder: LLMPromptBuilder):
            self._llm_client = llm_client
            self._prompt_builder = prompt_builder

        async def process(self, context: PipelineContext) -> PipelineContext:
            """Generate response using LLM with retrieved context."""
            # Build prompt, call LLM, store response
            # Track token usage in context metadata

        def should_skip(self, context: PipelineContext) -> bool:
            """Skip if already have final_response (moderated/canned)."""
            return context.final_response is not None
    ```

### Task 27d.5: Create No-Context Fallback Handler

- [ ] Handle cases with no relevant QA pairs (1 file, ~40 LOC)
  - **Tests**: `tests/unit/test_no_context_handler.py`
    - `test_generates_admission_response()` - "I don't have info on..."
    - `test_suggests_rephrasing()` - helpful guidance
    - `test_configurable_fallback_message()` - deployment-specific
  - **Files**: `backend/pipeline/processors/no_context_handler.py`
  - **Implementation**:
    ```python
    class NoContextHandler:
        def __init__(self, fallback_template: str):
            self._fallback_template = fallback_template

        def should_use_fallback(self, context: PipelineContext) -> bool:
            """Check if no relevant context was found."""
            return len(context.retrieved_qa_pairs) == 0

        def get_fallback_response(self, context: PipelineContext) -> str:
            """Generate polite admission of no knowledge."""
    ```

---

## 3. Success Criteria

- [ ] Similarity search returns ranked QA pairs above threshold
- [ ] RAG retrieval uses translated message for non-English
- [ ] RAG retrieval skips for moderated/greeting messages
- [ ] LLM prompt includes system prompt, context, history
- [ ] LLM generation tracks token usage for cost
- [ ] No-context cases get polite fallback, not hallucination
- [ ] Pipeline continues gracefully when LLM fails
- [ ] All tests pass: `pytest tests/unit/test_similarity_*.py tests/unit/test_rag_*.py tests/unit/test_llm_*.py tests/unit/test_no_context_*.py -v`
- [ ] Coverage ≥80% for RAG and LLM processors
