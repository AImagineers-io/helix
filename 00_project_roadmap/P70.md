# P70: Improvement Plan for P63.md

## Target Phase

P63: Import API Endpoints

---

## What's Weak / Missing

- **No authentication specified**: Endpoints allow anyone to import; should require admin API key
- **Missing row count limit**: File size limited to 10MB but no limit on row count; malicious user uploads 100k-row file
- **No rate limiting**: User can spam import requests, consuming resources
- **No cancel endpoint**: Once started, import cannot be stopped
- **Missing file content validation**: Extension checked but not content; malicious polyglot files accepted
- **No webhook/callback for completion**: Client must poll; no push notification option
- **Error report only CSV**: No JSON option for programmatic consumption
- **Filename not sanitized**: Path traversal characters, unicode attacks possible
- **No API versioning**: Breaking changes will affect all clients
- **Missing OpenAPI schema annotations**: Auto-generated docs incomplete
- **No concurrent import limit**: User can start unlimited simultaneous imports
- **No checksum/deduplication**: Re-uploading same file creates duplicate job

---

## Why This Matters

- **Security vulnerability**: Unauthenticated import endpoint allows anyone to pollute knowledge base
- **Resource exhaustion**: No rate limiting or concurrent limits enables DoS via import spam
- **Operational friction**: No cancel means stuck imports require DB intervention
- **Integration complexity**: Polling-only completion detection complicates automation pipelines
- **Attack surface**: Unsanitized filenames enable path traversal in error reports or logs

---

## Proposed Improvements

### 1. Add Authentication

```python
from backend.auth import require_api_key, AdminUser

@router.post("/qa/import/csv", response_model=ImportResponse, status_code=202)
async def import_csv(
    file: UploadFile = File(...),
    background_tasks: BackgroundTasks,
    session: Session = Depends(get_db),
    current_user: AdminUser = Depends(require_api_key),  # Add auth
):
    """Upload CSV file for QA import. Requires admin API key."""
    job_id = import_service.start_import(
        filename=file.filename,
        content=content,
        source_type="csv",
        created_by=current_user.identifier,  # Track who imported
    )
    # ...
```

Add tests:
- `test_import_requires_api_key()`
- `test_import_rejects_invalid_key()`
- `test_import_tracks_created_by()`

### 2. Add Row Count Limit

```python
class ImportConfig:
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
    MAX_ROWS = 10000

    @classmethod
    def from_env(cls):
        return cls(
            MAX_FILE_SIZE=int(os.getenv("IMPORT_MAX_FILE_SIZE", 10 * 1024 * 1024)),
            MAX_ROWS=int(os.getenv("IMPORT_MAX_ROWS", 10000)),
        )

async def import_csv(...):
    # Quick row count estimate before full processing
    estimated_rows = estimate_row_count(content, "csv")
    if estimated_rows > config.MAX_ROWS:
        raise HTTPException(
            400,
            f"File exceeds maximum {config.MAX_ROWS} rows (estimated: {estimated_rows})"
        )
```

Add test: `test_import_rejects_exceeding_row_limit()`

### 3. Add Rate Limiting

```python
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@router.post("/qa/import/csv", response_model=ImportResponse, status_code=202)
@limiter.limit("5/minute")  # Max 5 imports per minute per IP
async def import_csv(...):
    ...

# Also limit by API key
@limiter.limit("10/hour", key_func=lambda: current_user.identifier)
```

Add test: `test_import_rate_limited()`

### 4. Add Cancel Endpoint

```python
@router.post("/qa/import/{job_id}/cancel", response_model=CancelResponse)
async def cancel_import(
    job_id: UUID,
    reason: str = Body(None, embed=True),
    session: Session = Depends(get_db),
    current_user: AdminUser = Depends(require_api_key),
):
    """Cancel an in-progress import job."""
    job = job_repository.get_by_id(job_id)
    if not job:
        raise HTTPException(404, "Import job not found")

    if job.status not in (ImportJobStatus.PENDING, ImportJobStatus.PROCESSING):
        raise HTTPException(
            400,
            f"Cannot cancel job in {job.status.value} status"
        )

    success = job_repository.cancel(job_id, reason)
    return CancelResponse(
        job_id=job_id,
        cancelled=success,
        message="Import cancelled" if success else "Failed to cancel"
    )
```

Add tests:
- `test_cancel_pending_job()`
- `test_cancel_processing_job()`
- `test_cannot_cancel_completed_job()`

### 5. Add Content Type Validation

```python
import magic

async def validate_upload(file: UploadFile, allowed_types: List[str]) -> bytes:
    """Validate file content matches declared type."""
    content = await file.read()

    # Check magic bytes
    detected_type = magic.from_buffer(content[:2048], mime=True)

    ALLOWED_MIME_TYPES = {
        ".csv": ["text/csv", "text/plain", "application/csv"],
        ".json": ["application/json", "text/plain"],
        ".txt": ["text/plain"],
        ".xlsx": ["application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"],
    }

    ext = Path(file.filename).suffix.lower()
    if ext not in ALLOWED_MIME_TYPES:
        raise HTTPException(400, f"Unsupported file type: {ext}")

    if detected_type not in ALLOWED_MIME_TYPES[ext]:
        raise HTTPException(
            400,
            f"File content type ({detected_type}) doesn't match extension ({ext})"
        )

    return content
```

Add test: `test_rejects_spoofed_file_type()`

### 6. Add Webhook Notification

```python
class ImportResponse(BaseModel):
    job_id: UUID
    status: str
    message: str
    webhook_url: Optional[str] = None  # If provided, will POST on completion

@router.post("/qa/import/csv", ...)
async def import_csv(
    file: UploadFile = File(...),
    webhook_url: Optional[str] = Query(None),
    webhook_secret: Optional[str] = Query(None),
    ...
):
    """Upload CSV. Optionally provide webhook_url for completion notification."""
    job_id = import_service.start_import(
        ...,
        webhook_url=webhook_url,
        webhook_secret=webhook_secret,
    )

# In ImportService, after completion:
async def _notify_webhook(self, job: ImportJob):
    if job.webhook_url:
        payload = {
            "job_id": str(job.id),
            "status": job.status.value,
            "success_count": job.success_count,
            "error_count": job.error_count,
            "completed_at": job.completed_at.isoformat(),
        }

        if job.webhook_secret:
            signature = hmac.new(
                job.webhook_secret.encode(),
                json.dumps(payload).encode(),
                hashlib.sha256
            ).hexdigest()
            headers = {"X-Webhook-Signature": signature}

        async with httpx.AsyncClient() as client:
            await client.post(job.webhook_url, json=payload, headers=headers)
```

Add tests:
- `test_webhook_called_on_completion()`
- `test_webhook_includes_signature()`

### 7. Add JSON Error Report

```python
@router.get("/qa/import/{job_id}/errors")
async def download_error_report(
    job_id: UUID,
    format: str = Query("csv", regex="^(csv|json)$"),
    session: Session = Depends(get_db),
):
    """Download error report as CSV or JSON."""
    job = job_repository.get_by_id(job_id)
    if not job:
        raise HTTPException(404, "Import job not found")

    if format == "json":
        return JSONResponse(
            content={"errors": job.errors, "total": len(job.errors)},
            headers={"Content-Disposition": f"attachment; filename=errors-{job_id}.json"}
        )
    else:
        csv_content = generate_error_csv(job.errors)
        return StreamingResponse(
            io.StringIO(csv_content),
            media_type="text/csv",
            headers={"Content-Disposition": f"attachment; filename=errors-{job_id}.csv"}
        )
```

Add test: `test_error_report_json_format()`

### 8. Sanitize Filename

```python
import re
import unicodedata

def sanitize_filename(filename: str) -> str:
    """Sanitize filename to prevent path traversal and encoding attacks."""
    # Normalize unicode
    filename = unicodedata.normalize("NFKC", filename)

    # Remove path separators
    filename = filename.replace("/", "_").replace("\\", "_")

    # Remove null bytes
    filename = filename.replace("\x00", "")

    # Keep only safe characters
    filename = re.sub(r'[^\w\s\-\.]', '_', filename)

    # Limit length
    name, ext = os.path.splitext(filename)
    if len(name) > 100:
        name = name[:100]

    return f"{name}{ext}"
```

Add tests:
- `test_sanitizes_path_traversal()`
- `test_sanitizes_unicode_attacks()`

### 9. Add Concurrent Import Limit

```python
MAX_CONCURRENT_IMPORTS = 3  # Per API key

@router.post("/qa/import/csv", ...)
async def import_csv(...):
    # Check concurrent imports
    active_count = job_repository.count_active(created_by=current_user.identifier)
    if active_count >= MAX_CONCURRENT_IMPORTS:
        raise HTTPException(
            429,
            f"Maximum {MAX_CONCURRENT_IMPORTS} concurrent imports allowed. "
            f"Wait for existing imports to complete."
        )
```

Add test: `test_limits_concurrent_imports()`

### 10. Add File Deduplication

```python
import hashlib

@router.post("/qa/import/csv", ...)
async def import_csv(
    file: UploadFile = File(...),
    force: bool = Query(False),  # Skip deduplication check
    ...
):
    content = await file.read()
    file_hash = hashlib.sha256(content).hexdigest()

    if not force:
        existing = job_repository.find_by_hash(file_hash)
        if existing and existing.status == ImportJobStatus.COMPLETED:
            return ImportResponse(
                job_id=existing.id,
                status="duplicate",
                message=f"File already imported in job {existing.id}. Use force=true to reimport."
            )
```

Add test: `test_detects_duplicate_file()`

### 11. Add OpenAPI Annotations

```python
@router.post(
    "/qa/import/csv",
    response_model=ImportResponse,
    status_code=202,
    summary="Import QA pairs from CSV",
    description="""
    Upload a CSV file to import QA pairs into the knowledge base.

    **CSV Format:**
    - Required columns: `question` (or `q`), `answer` (or `a`)
    - Optional columns: `category`, `tags` (comma-separated)

    **Limits:**
    - Max file size: 10MB
    - Max rows: 10,000
    - Rate limit: 5 imports/minute
    """,
    responses={
        202: {"description": "Import started, returns job ID"},
        400: {"description": "Invalid file or format"},
        401: {"description": "Missing or invalid API key"},
        413: {"description": "File too large"},
        429: {"description": "Rate limit or concurrent limit exceeded"},
    },
    tags=["Import"],
)
async def import_csv(...):
    ...
```

---

## Impact on Roadmap

| Affected Phase | Change |
|----------------|--------|
| P59 | Add `webhook_url`, `webhook_secret`, `file_hash` to ImportJob model |
| P62 | Import service calls webhook on completion |
| P64 | Show cancel button, add force reimport option |
| P65 | Add security tests, rate limit tests |

---

## New Tasks for P63

- **Task 63.7**: Add API key authentication to all import endpoints
- **Task 63.8**: Implement row count limit with pre-parse estimation
- **Task 63.9**: Add rate limiting with slowapi
- **Task 63.10**: Create cancel endpoint with job status validation
- **Task 63.11**: Add magic byte content validation
- **Task 63.12**: Implement webhook notification on job completion
- **Task 63.13**: Add JSON format option for error reports
- **Task 63.14**: Implement filename sanitization
- **Task 63.15**: Add concurrent import limit per user
- **Task 63.16**: Add file hash deduplication
- **Task 63.17**: Complete OpenAPI documentation with examples

---

## Dependency Changes

Add to `backend/requirements.txt`:
```
slowapi>=0.1.8         # Rate limiting
python-magic>=0.4.27   # Content type detection
```
