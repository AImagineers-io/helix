# P38: Improvement Plan for P31.md

## What's Weak / Missing

- **No overloaded_error handling**: Anthropic returns `overloaded_error` when capacity is limited. Not mapped or retried.
- **Context formatting differs from OpenAI**: P30 uses `Context:\n{context}\n\nQuestion:` but P31 uses same format inconsistently. Should be unified.
- **Temperature range not validated**: Anthropic temperature is 0-1, but some callers might pass higher values (OpenAI allows up to 2).
- **Missing max_tokens validation**: Anthropic has model-specific limits. Exceeding causes API error.
- **History parameter not implemented**: P36 adds history to interface; P31 doesn't use it.
- **No timeout configuration**: Unlike OpenAI provider, Anthropic client doesn't receive timeout config.
- **Model version handling unclear**: Claude model names include dates. No guidance on version updates.
- **Retry doesn't extract retry-after from Anthropic headers**: Anthropic sends `retry-after` header too.

---

## Why This Matters

Overloaded errors are common during high-traffic periods. Inconsistent context formatting causes different response quality between providers. Temperature > 1 causes API errors. Missing history breaks multi-turn conversations through fallback.

---

## Proposed Improvements

### Task 38.1: Handle Overloaded Error

- [ ] Map and retry overloaded_error (1 file modification, ~20 LOC)
  - **Tests**: `tests/unit/test_anthropic_provider.py`
    - `test_overloaded_error_retried()`
    - `test_overloaded_error_mapped_to_rate_limit()`
  - **Files**: `backend/llm/providers/anthropic.py`
  - **Implementation**:
    ```python
    async def _with_retry(self, operation: Callable) -> Any:
        last_error = None
        for attempt in range(self.config.max_retries):
            try:
                return await operation()
            except (anthropic.RateLimitError, anthropic.APIStatusError) as e:
                # APIStatusError with status 529 is overloaded
                if isinstance(e, anthropic.APIStatusError) and e.status_code != 529:
                    raise
                last_error = e
                retry_after = getattr(e, 'headers', {}).get('retry-after', 0)
                delay = max(float(retry_after or 0), (2 ** attempt) + random.uniform(0, 1))
                logger.warning(f"Anthropic rate/capacity limit, attempt {attempt + 1}, retrying in {delay:.1f}s")
                await asyncio.sleep(delay)
        raise RateLimitError()
    ```

### Task 38.2: Unify Context Formatting

- [ ] Use consistent context format with OpenAI (1 file modification, ~10 LOC)
  - **Tests**: `tests/unit/test_anthropic_provider.py`
    - `test_context_format_matches_openai()`
  - **Files**: `backend/llm/providers/anthropic.py`
  - **Implementation**:
    ```python
    async def generate(
        self,
        system_prompt: str,
        user_message: str,
        context: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,
        max_tokens: int = 1024,
        temperature: float = 0.7,
    ) -> LLMResponse:
        # Build messages with history
        messages = []
        if history:
            messages.extend(history)

        # Combine context with user message (same format as OpenAI)
        content = user_message
        if context:
            content = f"Context:\n{context}\n\nQuestion: {user_message}"
        messages.append({"role": "user", "content": content})
        ...
    ```

### Task 38.3: Validate Temperature and Max Tokens

- [ ] Clamp values to Anthropic limits (1 file modification, ~15 LOC)
  - **Tests**: `tests/unit/test_anthropic_provider.py`
    - `test_temperature_clamped_to_1()`
    - `test_max_tokens_clamped_to_model_limit()`
  - **Files**: `backend/llm/providers/anthropic.py`
  - **Implementation**:
    ```python
    # Model-specific limits
    ANTHROPIC_MAX_TOKENS = {
        "claude-3-haiku-20240307": 4096,
        "claude-3-5-sonnet-20241022": 8192,
        "claude-3-opus-20240229": 4096,
    }

    async def generate(...) -> LLMResponse:
        # Clamp temperature to Anthropic's 0-1 range
        temperature = min(max(temperature, 0.0), 1.0)

        # Clamp max_tokens to model limit
        model_limit = ANTHROPIC_MAX_TOKENS.get(self.config.model, 4096)
        max_tokens = min(max_tokens, model_limit)

        response = await self.client.messages.create(
            model=self.config.model,
            system=system_prompt,
            messages=messages,
            max_tokens=max_tokens,
            temperature=temperature,
        )
        ...
    ```

### Task 38.4: Add Timeout Configuration

- [ ] Pass timeout to AsyncAnthropic client (1 file modification, ~10 LOC)
  - **Tests**: `tests/unit/test_anthropic_provider.py`
    - `test_client_uses_configured_timeout()`
  - **Files**: `backend/llm/providers/anthropic.py`
  - **Implementation**:
    ```python
    from anthropic import AsyncAnthropic, Timeout

    class AnthropicProvider(LLMProvider):
        def __init__(self, config: AnthropicConfig):
            self.config = config
            self.client = AsyncAnthropic(
                api_key=config.api_key,
                timeout=Timeout(
                    connect=5.0,
                    read=config.timeout,
                    write=10.0,
                    pool=5.0,
                ),
            )
    ```

### Task 38.5: Add Model Version Config

- [ ] Externalize model version for easy updates (1 file modification, ~15 LOC)
  - **Tests**: `tests/unit/test_llm_config.py`
    - `test_anthropic_model_configurable_via_env()`
  - **Files**: `backend/llm/config.py`
  - **Implementation**:
    ```python
    @dataclass
    class AnthropicConfig(ProviderConfig):
        model: str = field(default_factory=lambda: os.getenv(
            "ANTHROPIC_MODEL", "claude-3-haiku-20240307"
        ))

        @classmethod
        def from_env(cls) -> "AnthropicConfig":
            return cls(
                api_key=os.getenv("ANTHROPIC_API_KEY", ""),
                model=os.getenv("ANTHROPIC_MODEL", "claude-3-haiku-20240307"),
                timeout=float(os.getenv("ANTHROPIC_TIMEOUT", "30.0")),
            )
    ```

---

## Impact on Roadmap

- **P32**: FallbackOrchestrator receives consistent response format from both providers.
- **P35**: Token limits now provider-aware via ANTHROPIC_MAX_TOKENS.
- **Configuration**: New env vars: `ANTHROPIC_MODEL`, `ANTHROPIC_TIMEOUT`.
- **Testing**: Temperature > 1 tests should verify clamping behavior.
