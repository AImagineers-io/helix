# P31: Anthropic Provider

## 1. Objective

**What**: Implement a fallback LLM provider using Anthropic's Claude API. When OpenAI fails, the system can automatically try Anthropic.

**Why**: Different infrastructure means uncorrelated failures. Claude models are comparable quality. Provides redundancy for critical production use.

**Scope**:
- Included: Anthropic provider class, message format handling, rate limit handling
- Excluded: Embeddings (Anthropic doesn't offer them), fallback orchestration (P32)

**Dependencies**: P29 (Provider Abstraction)

---

## 2. Implementation Tasks

### Task 31.1: Anthropic Provider Implementation

- [ ] Implement Anthropic provider class (1 file, ~100 LOC)
  - **Tests**: `tests/unit/test_anthropic_provider.py`
    - `test_generate_returns_llm_response()`
    - `test_generate_uses_correct_model()`
    - `test_generate_formats_system_prompt_correctly()`
    - `test_generate_includes_context_in_message()`
    - `test_generate_respects_max_tokens()`
    - `test_embed_raises_not_supported()`
    - `test_supports_embeddings_returns_false()`
  - **Files**: `backend/llm/providers/anthropic.py`
  - **Implementation**:
    ```python
    from anthropic import AsyncAnthropic
    from backend.llm.base import LLMProvider, LLMResponse, EmbeddingResponse
    from backend.llm.config import AnthropicConfig
    from backend.llm.errors import NotSupportedError

    class AnthropicProvider(LLMProvider):
        def __init__(self, config: AnthropicConfig):
            self.config = config
            self.client = AsyncAnthropic(api_key=config.api_key)

        async def generate(
            self,
            system_prompt: str,
            user_message: str,
            context: Optional[str] = None,
            max_tokens: int = 1024,
            temperature: float = 0.7,
        ) -> LLMResponse:
            content = user_message
            if context:
                content = f"Context:\n{context}\n\nQuestion: {user_message}"

            response = await self.client.messages.create(
                model=self.config.model,
                system=system_prompt,
                messages=[{"role": "user", "content": content}],
                max_tokens=max_tokens,
                temperature=temperature,
            )
            return LLMResponse(
                content=response.content[0].text,
                model=response.model,
                input_tokens=response.usage.input_tokens,
                output_tokens=response.usage.output_tokens,
                provider="anthropic",
            )

        async def embed(self, texts: List[str]) -> EmbeddingResponse:
            raise NotSupportedError("Anthropic does not support embeddings")

        @property
        def supports_embeddings(self) -> bool:
            return False
    ```

### Task 31.2: Rate Limit Handling

- [ ] Add exponential backoff for Anthropic rate limits (1 file modification, ~40 LOC)
  - **Tests**: `tests/unit/test_anthropic_provider.py`
    - `test_retries_on_rate_limit()`
    - `test_raises_after_max_retries()`
    - `test_exponential_backoff_increases_delay()`
  - **Files**: `backend/llm/providers/anthropic.py`
  - **Implementation**:
    ```python
    async def _with_retry(self, operation: Callable) -> Any:
        last_error = None
        for attempt in range(self.config.max_retries):
            try:
                return await operation()
            except anthropic.RateLimitError as e:
                last_error = e
                delay = (2 ** attempt) + random.uniform(0, 1)
                await asyncio.sleep(delay)
        raise RateLimitError()
    ```

### Task 31.3: Error Mapping

- [ ] Map Anthropic errors to provider errors (1 file modification, ~30 LOC)
  - **Tests**: `tests/unit/test_anthropic_provider.py`
    - `test_maps_auth_error()`
    - `test_maps_timeout_error()`
    - `test_maps_api_error()`
  - **Files**: `backend/llm/providers/anthropic.py`
  - **Implementation**:
    ```python
    def _map_error(self, error: Exception) -> LLMError:
        if isinstance(error, anthropic.AuthenticationError):
            return AuthenticationError("Invalid Anthropic API key")
        if isinstance(error, anthropic.APITimeoutError):
            return TimeoutError(self.config.timeout)
        if isinstance(error, anthropic.RateLimitError):
            return RateLimitError()
        return LLMError(str(error))
    ```

### Task 31.4: Register Provider

- [ ] Register Anthropic with provider registry (1 file modification, ~5 LOC)
  - **Tests**: `tests/unit/test_provider_registry.py`
    - `test_anthropic_registered_by_default()`
  - **Files**: `backend/llm/__init__.py`
  - **Implementation**:
    ```python
    from backend.llm.providers.anthropic import AnthropicProvider

    ProviderRegistry.register("anthropic", AnthropicProvider)
    ```

### Task 31.5: Optional Configuration

- [ ] Make Anthropic config optional (1 file modification, ~20 LOC)
  - **Tests**: `tests/unit/test_llm_config.py`
    - `test_anthropic_config_optional()`
    - `test_system_works_without_anthropic()`
  - **Files**: `backend/llm/config.py`
  - **Implementation**:
    ```python
    @dataclass
    class LLMConfig:
        openai: OpenAIConfig
        anthropic: Optional[AnthropicConfig] = None

        @classmethod
        def from_env(cls) -> "LLMConfig":
            anthropic_key = os.getenv("ANTHROPIC_API_KEY")
            return cls(
                openai=OpenAIConfig.from_env(),
                anthropic=AnthropicConfig.from_env() if anthropic_key else None,
            )
    ```

---

## 3. Success Criteria

- [ ] Anthropic provider implements LLMProvider interface
- [ ] Generate returns properly formatted LLMResponse
- [ ] Embed raises NotSupportedError (Anthropic limitation)
- [ ] Rate limits handled with exponential backoff
- [ ] Anthropic errors mapped to provider error types
- [ ] Provider works without Anthropic API key configured
- [ ] All tests pass with mocked Anthropic client
