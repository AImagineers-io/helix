# P33: Response Cache

## 1. Objective

**What**: Cache LLM responses in Redis keyed by hash of prompt and context. Identical queries return cached responses instantly without LLM costs.

**Why**: LLM calls are expensive (~$0.001-0.01 per call). Many queries are repetitive (FAQs). Cache hits are instant vs 1-2 second LLM latency. Target: 30%+ cache hit rate.

**Scope**:
- Included: Response cache service, cache key generation, TTL management, cache invalidation
- Excluded: Cost tracking (P34), token management (P35)

**Dependencies**: P32 (Fallback Logic), Redis infrastructure

---

## 2. Implementation Tasks

### Task 33.1: Response Cache Service

- [ ] Create response caching service (1 file, ~80 LOC)
  - **Tests**: `tests/unit/test_response_cache.py`
    - `test_cache_miss_returns_none()`
    - `test_cache_hit_returns_response()`
    - `test_cache_stores_response()`
    - `test_cache_respects_ttl()`
    - `test_cache_key_includes_system_prompt()`
    - `test_cache_key_includes_context()`
    - `test_cache_key_includes_user_message()`
  - **Files**: `backend/llm/cache.py`
  - **Implementation**:
    ```python
    import hashlib
    import json
    from redis.asyncio import Redis
    from backend.llm.models import LLMResponse

    @dataclass
    class CacheConfig:
        ttl_seconds: int = 86400  # 24 hours
        prefix: str = "llm:response"
        enabled: bool = True

    class ResponseCache:
        def __init__(self, redis: Redis, config: CacheConfig):
            self.redis = redis
            self.config = config

        def _generate_key(
            self,
            system_prompt: str,
            user_message: str,
            context: Optional[str],
        ) -> str:
            content = json.dumps({
                "system": system_prompt,
                "user": user_message,
                "context": context or "",
            }, sort_keys=True)
            hash_value = hashlib.sha256(content.encode()).hexdigest()[:16]
            return f"{self.config.prefix}:{hash_value}"

        async def get(
            self,
            system_prompt: str,
            user_message: str,
            context: Optional[str],
        ) -> Optional[LLMResponse]:
            if not self.config.enabled:
                return None
            key = self._generate_key(system_prompt, user_message, context)
            data = await self.redis.get(key)
            if data:
                return LLMResponse(**json.loads(data))
            return None

        async def set(
            self,
            system_prompt: str,
            user_message: str,
            context: Optional[str],
            response: LLMResponse,
        ) -> None:
            if not self.config.enabled:
                return
            key = self._generate_key(system_prompt, user_message, context)
            data = json.dumps(response.__dict__)
            await self.redis.setex(key, self.config.ttl_seconds, data)
    ```

### Task 33.2: Cache Integration with Fallback

- [ ] Integrate cache with fallback orchestrator (1 file, ~50 LOC)
  - **Tests**: `tests/unit/test_cached_llm_service.py`
    - `test_returns_cached_response_on_hit()`
    - `test_calls_provider_on_cache_miss()`
    - `test_stores_response_after_generation()`
    - `test_marks_cached_response()`
    - `test_cache_used_as_last_fallback()`
  - **Files**: `backend/llm/service.py`
  - **Implementation**:
    ```python
    class LLMService:
        def __init__(
            self,
            orchestrator: FallbackOrchestrator,
            cache: ResponseCache,
        ):
            self.orchestrator = orchestrator
            self.cache = cache

        async def generate(
            self,
            system_prompt: str,
            user_message: str,
            context: Optional[str] = None,
            **kwargs,
        ) -> LLMResponse:
            # Check cache first
            cached = await self.cache.get(system_prompt, user_message, context)
            if cached:
                cached.cached = True
                return cached

            # Try providers
            try:
                response = await self.orchestrator.generate(
                    system_prompt, user_message, context, **kwargs
                )
                # Cache successful response
                await self.cache.set(system_prompt, user_message, context, response)
                return response
            except AllProvidersFailedError:
                # Last resort: check cache even for similar queries
                return await self._graceful_fallback(user_message)
    ```

### Task 33.3: Cache Invalidation

- [ ] Add cache invalidation on prompt changes (1 file modification, ~30 LOC)
  - **Tests**: `tests/unit/test_response_cache.py`
    - `test_invalidate_by_prefix()`
    - `test_invalidate_all()`
    - `test_invalidation_returns_count()`
  - **Files**: `backend/llm/cache.py`
  - **Implementation**:
    ```python
    async def invalidate_prefix(self, prefix: str) -> int:
        """Invalidate all cache entries matching prefix."""
        pattern = f"{self.config.prefix}:{prefix}*"
        keys = await self.redis.keys(pattern)
        if keys:
            return await self.redis.delete(*keys)
        return 0

    async def invalidate_all(self) -> int:
        """Clear entire response cache."""
        pattern = f"{self.config.prefix}:*"
        keys = await self.redis.keys(pattern)
        if keys:
            return await self.redis.delete(*keys)
        return 0
    ```

### Task 33.4: Cache Metrics

- [ ] Track cache hit/miss metrics (1 file modification, ~40 LOC)
  - **Tests**: `tests/unit/test_response_cache.py`
    - `test_tracks_hit_count()`
    - `test_tracks_miss_count()`
    - `test_calculates_hit_rate()`
  - **Files**: `backend/llm/cache.py`
  - **Implementation**:
    ```python
    @dataclass
    class CacheMetrics:
        hits: int = 0
        misses: int = 0

        @property
        def hit_rate(self) -> float:
            total = self.hits + self.misses
            return self.hits / total if total > 0 else 0.0

    class ResponseCache:
        def __init__(self, ...):
            self.metrics = CacheMetrics()

        async def get(self, ...) -> Optional[LLMResponse]:
            ...
            if data:
                self.metrics.hits += 1
                return LLMResponse(**json.loads(data))
            self.metrics.misses += 1
            return None
    ```

---

## 3. Success Criteria

- [ ] Cache stores responses keyed by prompt+context hash
- [ ] Cache hits return instantly without LLM call
- [ ] Cache misses call provider and store result
- [ ] 24-hour TTL by default (configurable)
- [ ] Cache invalidation works on prompt changes
- [ ] Hit/miss metrics tracked
- [ ] Cached responses marked with `cached=True`
- [ ] All tests pass: `pytest tests/unit/test_response_cache.py tests/unit/test_cached_llm_service.py -v`
