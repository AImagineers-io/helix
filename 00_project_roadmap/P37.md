# P37: Improvement Plan for P30.md

## What's Weak / Missing

- **Timeout not passed to AsyncOpenAI client**: Config has `timeout` field but it's never used in client instantiation.
- **Context as separate message is problematic**: Adding context as a separate user message creates confusing conversation structure. Should be part of the user message or system prompt.
- **No content filter error handling**: OpenAI returns specific errors when content is flagged. Not mapped to `ContentFilterError`.
- **Missing test for null/empty response content**: `response.choices[0].message.content` can be None.
- **Retry logic doesn't log attempts**: Silent retries make debugging difficult.
- **No test for network errors vs API errors**: Connection refused, DNS failures not handled distinctly.
- **History parameter not implemented**: P36 adds history to interface; P30 doesn't use it.
- **No request timeout distinction**: Connection timeout vs read timeout not configurable.

---

## Why This Matters

Timeouts without enforcement mean requests hang forever. Silent retries hide problems until they cascade. Null response content causes AttributeError in production. Content filter errors need different handling than rate limits (no retry helps).

---

## Proposed Improvements

### Task 37.1: Pass Timeout to Client

- [ ] Configure timeout in AsyncOpenAI client (1 file modification, ~10 LOC)
  - **Tests**: `tests/unit/test_openai_provider.py`
    - `test_client_uses_configured_timeout()`
    - `test_timeout_error_raised_after_duration()`
  - **Files**: `backend/llm/providers/openai.py`
  - **Implementation**:
    ```python
    from openai import AsyncOpenAI, Timeout

    class OpenAIProvider(LLMProvider):
        def __init__(self, config: OpenAIConfig):
            self.config = config
            self.client = AsyncOpenAI(
                api_key=config.api_key,
                timeout=Timeout(
                    connect=5.0,
                    read=config.timeout,
                    write=10.0,
                    pool=5.0,
                ),
            )
    ```

### Task 37.2: Fix Context Handling

- [ ] Include context in user message, not as separate message (1 file modification, ~15 LOC)
  - **Tests**: `tests/unit/test_openai_provider.py`
    - `test_context_included_in_user_message()`
    - `test_message_structure_correct()`
  - **Files**: `backend/llm/providers/openai.py`
  - **Implementation**:
    ```python
    async def generate(
        self,
        system_prompt: str,
        user_message: str,
        context: Optional[str] = None,
        history: Optional[List[Dict[str, str]]] = None,
        max_tokens: int = 1024,
        temperature: float = 0.7,
    ) -> LLMResponse:
        messages = [{"role": "system", "content": system_prompt}]

        # Add conversation history
        if history:
            messages.extend(history)

        # Combine context with user message
        content = user_message
        if context:
            content = f"Context:\n{context}\n\nQuestion: {user_message}"
        messages.append({"role": "user", "content": content})
        ...
    ```

### Task 37.3: Handle Content Filter Errors

- [ ] Map content_filter finish_reason to error (1 file modification, ~20 LOC)
  - **Tests**: `tests/unit/test_openai_provider.py`
    - `test_content_filter_raises_error()`
    - `test_content_filter_error_includes_reason()`
  - **Files**: `backend/llm/providers/openai.py`
  - **Implementation**:
    ```python
    async def generate(...) -> LLMResponse:
        response = await self._with_retry(lambda: self.client.chat.completions.create(...))

        choice = response.choices[0]
        if choice.finish_reason == "content_filter":
            raise ContentFilterError("Response blocked by OpenAI content filter")

        content = choice.message.content
        if content is None:
            content = ""  # Handle null content gracefully

        return LLMResponse(content=content, ...)
    ```

### Task 37.4: Add Retry Logging

- [ ] Log retry attempts with context (1 file modification, ~15 LOC)
  - **Tests**: `tests/unit/test_openai_provider.py`
    - `test_retry_attempts_logged()`
    - `test_final_failure_logged_with_attempts()`
  - **Files**: `backend/llm/providers/openai.py`
  - **Implementation**:
    ```python
    async def _with_retry(self, operation: Callable) -> Any:
        last_error = None
        for attempt in range(self.config.max_retries):
            try:
                return await operation()
            except openai.RateLimitError as e:
                last_error = e
                retry_after = float(e.response.headers.get("retry-after", 0))
                delay = max(retry_after, (2 ** attempt) + random.uniform(0, 1))
                logger.warning(
                    f"OpenAI rate limit hit, attempt {attempt + 1}/{self.config.max_retries}, "
                    f"retrying in {delay:.1f}s"
                )
                await asyncio.sleep(delay)

        logger.error(f"OpenAI failed after {self.config.max_retries} attempts: {last_error}")
        raise RateLimitError(retry_after=delay)
    ```

### Task 37.5: Handle Network Errors

- [ ] Map connection errors to appropriate LLMError (1 file modification, ~15 LOC)
  - **Tests**: `tests/unit/test_openai_provider.py`
    - `test_connection_error_mapped()`
    - `test_dns_error_mapped()`
  - **Files**: `backend/llm/providers/openai.py`
  - **Implementation**:
    ```python
    import httpx

    def _map_error(self, error: Exception) -> LLMError:
        if isinstance(error, openai.AuthenticationError):
            return AuthenticationError("Invalid OpenAI API key")
        if isinstance(error, openai.APITimeoutError):
            return TimeoutError(self.config.timeout)
        if isinstance(error, openai.RateLimitError):
            return RateLimitError()
        if isinstance(error, openai.APIConnectionError):
            return LLMError(f"Connection failed: {error}")
        if isinstance(error, httpx.ConnectError):
            return LLMError(f"Network error: {error}")
        return LLMError(str(error))
    ```

---

## Impact on Roadmap

- **P31**: Should follow same context handling pattern for consistency.
- **P32**: FallbackOrchestrator can now distinguish content filter errors (don't retry).
- **P34**: Request ID from response enables cost tracking correlation.
- **Observability**: Retry logging enables monitoring of provider health.
- **Dependencies**: Add `httpx` to imports for connection error handling.
