# P28d: Improvement Plan for P27d.md

## What's Weak / Missing

- **EmbeddingService undefined**: Referenced in SimilarityService but no definition or dependency link to where it's built.
- **Query embedding failure not handled**: If embedding generation fails for the user's query, similarity search cannot proceed. No fallback.
- **Token counting mechanism unspecified**: "respects token limits" mentioned but no tokenizer or counting logic defined.
- **NoContextHandler integration unclear**: It's a helper class, not a processor. How does it plug into the pipeline flow?
- **Embedding dimension mismatch unhandled**: Query embedding must match stored embeddings (1536 dims). No validation.
- **PromptService not listed as dependency**: LLMPromptBuilder requires it but P27d dependencies don't include the phase where it's defined.
- **No LLM response length control**: LLM can generate arbitrarily long responses. No max_tokens configuration.
- **Multi-turn history limit undefined**: "includes conversation history" but no limit on how many messages. Memory unbounded.
- **Streaming not considered**: Long responses block until complete. No streaming support for better UX.
- **Primary/fallback LLM failure both unhandled**: What response does user get if OpenAI AND Anthropic both fail?

---

## Why This Matters

RAG and LLM are the core value of Helix. Embedding failures mean no relevant context. Token limit violations cause truncated prompts that lose critical context. Unbounded history means memory exhaustion. Missing fallback handling means users see errors instead of graceful degradation.

---

## Proposed Improvements

### Task 28d.1: Define EmbeddingService Dependency

- [ ] Document EmbeddingService interface and source (1 file, ~50 LOC)
  - **Tests**: `tests/unit/test_embedding_service.py`
    - `test_generates_query_embedding()`
    - `test_embedding_has_correct_dimensions()`
    - `test_handles_api_failure_gracefully()`
    - `test_caches_recent_embeddings()`
  - **Files**: `backend/services/embedding_service.py`
  - **Implementation**:
    ```python
    class EmbeddingService:
        DIMENSIONS = 1536  # text-embedding-3-small

        def __init__(self, openai_client: OpenAI, cache: Optional[Redis] = None):
            self._client = openai_client
            self._cache = cache

        async def embed_query(self, text: str) -> Optional[List[float]]:
            """Generate embedding for search query. Returns None on failure."""

        async def embed_batch(self, texts: List[str]) -> List[Optional[List[float]]]:
            """Batch embedding for QA pair ingestion."""
    ```

### Task 28d.2: Handle Query Embedding Failure

- [ ] Graceful fallback when query cannot be embedded (1 file modification, ~25 LOC)
  - **Tests**: `tests/unit/test_similarity_search.py`
    - `test_embedding_failure_returns_empty_results()`
    - `test_embedding_failure_logged_with_context()`
  - **Files**: `backend/services/similarity_service.py`
  - **Implementation**:
    ```python
    async def search(self, query: str, ...) -> List[SimilarityResult]:
        query_embedding = await self._embedding_service.embed_query(query)
        if query_embedding is None:
            logger.warning(f"Failed to embed query: {query[:50]}...")
            return []  # Continue pipeline with no context
        ...
    ```

### Task 28d.3: Add Proper Token Counting

- [ ] Use tiktoken for accurate token counting (1 file modification, ~40 LOC)
  - **Tests**: `tests/unit/test_llm_prompt_builder.py`
    - `test_counts_tokens_accurately()`
    - `test_truncates_history_to_fit_limit()`
    - `test_truncates_context_to_fit_limit()`
    - `test_preserves_system_prompt_and_user_message()`
  - **Files**: `backend/pipeline/processors/llm_prompt_builder.py`
  - **Implementation**:
    ```python
    import tiktoken

    class LLMPromptBuilder:
        def __init__(self, ..., model: str = "gpt-4o-mini"):
            self._encoding = tiktoken.encoding_for_model(model)

        def _count_tokens(self, text: str) -> int:
            return len(self._encoding.encode(text))

        def _truncate_to_fit(self, messages: List[Message], max_tokens: int) -> List[Message]:
            # Preserve system + user, truncate history/context
    ```

### Task 28d.4: Integrate NoContextHandler as Processor

- [ ] Convert NoContextHandler to a proper processor (1 file modification, ~30 LOC)
  - **Tests**: `tests/unit/test_no_context_processor.py`
    - `test_activates_when_no_qa_pairs_retrieved()`
    - `test_sets_final_response_to_fallback()`
    - `test_skips_when_qa_pairs_exist()`
  - **Files**: `backend/pipeline/processors/no_context_handler.py`
  - **Implementation**:
    ```python
    class NoContextProcessor(Processor):
        name = "no_context_handler"
        is_optional = True  # If it fails, LLM can still try

        async def process(self, context: PipelineContext) -> PipelineContext:
            if len(context.retrieved_qa_pairs) == 0:
                return context.with_update(
                    generated_response=self._fallback_template,
                    final_response=self._fallback_template
                )
            return context

        def should_skip(self, context: PipelineContext) -> bool:
            return len(context.retrieved_qa_pairs) > 0
    ```

### Task 28d.5: Add Conversation History Limit

- [ ] Limit history to configurable message count (1 file modification, ~20 LOC)
  - **Tests**: `tests/unit/test_llm_prompt_builder.py`
    - `test_limits_history_to_max_messages()`
    - `test_keeps_most_recent_messages()`
  - **Files**: `backend/pipeline/processors/llm_prompt_builder.py`, `backend/pipeline/config.py`
  - **Config**:
    ```python
    max_history_messages: int = 10
    ```

### Task 28d.6: Add LLM Response Length Control

- [ ] Configure max_tokens for LLM responses (1 file modification, ~15 LOC)
  - **Tests**: `tests/unit/test_llm_generation_processor.py`
    - `test_respects_max_response_tokens()`
    - `test_config_drives_max_tokens()`
  - **Files**: `backend/pipeline/processors/llm_generation.py`, `backend/pipeline/config.py`
  - **Config**:
    ```python
    llm_max_response_tokens: int = 500
    ```

### Task 28d.7: Add Complete LLM Failure Fallback

- [ ] Define user-facing response when all LLM providers fail (1 file modification, ~25 LOC)
  - **Tests**: `tests/unit/test_llm_generation_processor.py`
    - `test_returns_fallback_on_complete_failure()`
    - `test_fallback_message_is_configurable()`
  - **Files**: `backend/pipeline/processors/llm_generation.py`
  - **Implementation**:
    ```python
    LLM_FAILURE_FALLBACK = (
        "I'm having trouble processing your request right now. "
        "Please try again in a moment."
    )

    async def process(self, context: PipelineContext) -> PipelineContext:
        try:
            response = await self._generate_with_fallback(...)
        except AllProvidersFailedError:
            logger.error("All LLM providers failed")
            return context.with_update(
                generated_response=self._failure_fallback,
                errors=context.errors + (ProcessorError(...),)
            )
    ```

---

## Impact on Roadmap

- **P27a**: Factory must instantiate EmbeddingService.
- **P27e**: Integration tests need to cover embedding failure and LLM failure scenarios.
- **Dependencies**: Add tiktoken to requirements.txt.
- **Pipeline config**: New fields for history limit, max response tokens.
- **Cost tracking**: Token counting now accurate for cost calculation.
