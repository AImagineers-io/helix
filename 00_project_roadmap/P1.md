# P1: Prompt Management Backend

## Source Reference

**PALAI Path**: `/mnt/d/codes/palai/`

Key files to modify:
- `backend/database/models.py` - Add PromptTemplate and PromptVersion models
- `backend/services/chat/orchestrator.py` - Load system prompt from database
- `backend/api/routers/` - Add new prompts router

Current prompt handling in PALAI:
- System prompts are hardcoded in `backend/services/chat/processors/retrieval_llm.py`
- No versioning or admin editing capability

---

## 1. Objective

- **What**: Database-stored prompt templates with versioning, CRUD API, and service layer.
- **Why**: Enables non-technical editing of LLM prompts and per-instance customization without code deploys.
- **Scope**:
  - **Included**: PromptTemplate model, PromptVersion model, CRUD endpoints, version history, publish/rollback
  - **Excluded**: Admin UI (P2), A/B traffic splitting logic, preview sandbox

---

## 2. Implementation Tasks

- [ ] Create PromptTemplate model (2 files, ~70 LOC)
  - Tests: tests/integration/test_prompt_template_model.py
  - Files: database/models.py, alembic/versions/xxx_create_prompt_template.py
  - PALAI ref: `backend/database/models.py` - add new model
  - Depends on: P0

- [ ] Create PromptVersion model (2 files, ~80 LOC)
  - Tests: tests/integration/test_prompt_version_model.py
  - Files: database/models.py, alembic/versions/xxx_create_prompt_version.py
  - PALAI ref: `backend/database/models.py` - add after PromptTemplate
  - Depends on: P1.1

- [ ] Create PromptService with version management (1 file, ~150 LOC)
  - Tests: tests/unit/test_prompt_service.py
  - Files: services/prompt_service.py (new)
  - PALAI ref: Pattern from `backend/services/backup_service.py`
  - Depends on: P1.2

- [ ] Implement prompt CRUD API endpoints (1 file, ~120 LOC)
  - Tests: tests/integration/test_prompt_api.py
  - Files: api/routers/prompts.py (new)
  - PALAI ref: Pattern from `backend/api/routers/qa_pairs.py`
  - Depends on: P1.3

- [ ] Add publish/rollback endpoints (1 file, ~60 LOC)
  - Tests: tests/integration/test_prompt_publish.py
  - Files: api/routers/prompts.py (extend)
  - Depends on: P1.4

- [ ] Seed default prompt templates (1 file, ~100 LOC)
  - Tests: tests/integration/test_prompt_seeding.py
  - Files: database/seeds/prompts.py (new)
  - PALAI ref: Extract prompts from `backend/services/chat/processors/retrieval_llm.py`
  - Depends on: P1.3

- [ ] Integrate prompts into chat orchestrator (2 files, ~80 LOC)
  - Tests: tests/integration/test_pipeline_uses_prompts.py
  - Files: services/chat/orchestrator.py, services/chat/processors/retrieval_llm.py
  - PALAI ref: `backend/services/chat/orchestrator.py:process()` - inject prompt loading
  - Depends on: P1.3, P1.6

---

## 3. Success Criteria

- [ ] PromptTemplate CRUD works via API (GET, POST, PUT, DELETE)
- [ ] Creating/updating a prompt creates new version automatically
- [ ] Only one version can be active per template at a time
- [ ] Publish endpoint activates a version, deactivates previous
- [ ] Rollback endpoint reverts to previous version
- [ ] Chat pipeline loads active system_prompt from database
- [ ] Default prompts seeded on fresh install
- [ ] Existing hardcoded prompts extracted and migrated
- [ ] API secured with ADMIN_API_KEY
