# P42: Improvement Plan for P35.md

## What's Weak / Missing

- **tiktoken only works for OpenAI models**: Claude models use different tokenization. Token counts will be inaccurate for Anthropic.
- **`_truncate_text` method undefined**: Called in truncation logic but never implemented.
- **No smart truncation**: Truncation cuts at token boundary, potentially mid-word or mid-sentence.
- **Model-specific context limits not configured**: gpt-4o-mini (128k), claude-3-haiku (200k) have different limits. Hardcoded 4000 is wrong.
- **Instance method calling class method incorrectly**: `self.counter.count()` but TokenCounter uses `@classmethod`.
- **No observability when truncation happens**: Only logs warning, doesn't emit event for monitoring.
- **Token counting adds latency**: Counting tokens before every request adds ~5-10ms. Should be optional or cached.
- **History truncation loses important context**: Oldest-first truncation may remove critical conversation setup.

---

## Why This Matters

Wrong token counts for Claude means either wasted context space or API errors. Undefined methods cause runtime crashes. Mid-word truncation creates gibberish context. Hardcoded limits waste 99% of available context window.

---

## Proposed Improvements

### Task 42.1: Add Claude Token Estimation

- [ ] Estimate Claude tokens (characters/4 heuristic) (1 file modification, ~25 LOC)
  - **Tests**: `tests/unit/test_token_counter.py`
    - `test_counts_claude_tokens_approximately()`
    - `test_uses_tiktoken_for_openai()`
    - `test_uses_estimation_for_unknown()`
  - **Files**: `backend/llm/tokens.py`
  - **Implementation**:
    ```python
    class TokenCounter:
        # Models that use tiktoken
        TIKTOKEN_MODELS = {"gpt-4o-mini", "gpt-4o", "gpt-4", "gpt-3.5-turbo"}

        @classmethod
        def count(cls, text: str, model: str = "gpt-4o-mini") -> int:
            # Use tiktoken for OpenAI models
            if any(model.startswith(m) for m in cls.TIKTOKEN_MODELS):
                encoding = cls._get_encoding(model)
                return len(encoding.encode(text))

            # Estimate for Claude and others (~4 chars per token)
            # This is conservative; Claude averages ~3.5 chars/token
            return len(text) // 4 + 1
    ```

### Task 42.2: Implement _truncate_text Method

- [ ] Add text truncation at word boundaries (1 file modification, ~30 LOC)
  - **Tests**: `tests/unit/test_context_truncation.py`
    - `test_truncate_text_at_word_boundary()`
    - `test_truncate_text_preserves_sentences()`
    - `test_truncate_text_respects_token_limit()`
  - **Files**: `backend/llm/truncation.py`
  - **Implementation**:
    ```python
    def _truncate_text(self, text: str, max_tokens: int, model: str) -> str:
        """Truncate text to fit within token limit at word boundary."""
        if not text:
            return ""

        current_tokens = TokenCounter.count(text, model)
        if current_tokens <= max_tokens:
            return text

        # Binary search for optimal truncation point
        words = text.split()
        low, high = 0, len(words)

        while low < high:
            mid = (low + high + 1) // 2
            candidate = " ".join(words[:mid])
            if TokenCounter.count(candidate, model) <= max_tokens:
                low = mid
            else:
                high = mid - 1

        truncated = " ".join(words[:low])
        return truncated + "..." if low < len(words) else truncated
    ```

### Task 42.3: Model-Specific Context Limits

- [ ] Configure limits per model (1 file modification, ~25 LOC)
  - **Tests**: `tests/unit/test_context_truncation.py`
    - `test_uses_model_specific_limit()`
    - `test_falls_back_to_default_limit()`
  - **Files**: `backend/llm/truncation.py`
  - **Implementation**:
    ```python
    # Context window limits (conservative, leaving room for output)
    MODEL_CONTEXT_LIMITS = {
        "gpt-4o-mini": 120000,
        "gpt-4o": 120000,
        "gpt-4": 8000,
        "claude-3-haiku-20240307": 190000,
        "claude-3-5-sonnet-20241022": 190000,
    }

    @dataclass
    class TruncationConfig:
        default_max_context: int = 4000  # Fallback for unknown models
        max_output_tokens: int = 1024
        reserve_for_output: int = 1024

        def get_max_context(self, model: str) -> int:
            return MODEL_CONTEXT_LIMITS.get(model, self.default_max_context)
    ```

### Task 42.4: Fix Class Method Usage

- [ ] Use TokenCounter correctly (1 file modification, ~10 LOC)
  - **Tests**: `tests/unit/test_context_truncation.py`
    - `test_truncator_uses_token_counter()`
  - **Files**: `backend/llm/truncation.py`
  - **Implementation**:
    ```python
    class ContextTruncator:
        def __init__(self, config: TruncationConfig):
            self.config = config
            # Remove: self.counter = TokenCounter()

        def truncate(self, ...) -> Tuple[str, List[Dict], bool]:
            available = self.config.get_max_context(model) - self.config.reserve_for_output

            # Use class method directly
            fixed_tokens = (
                TokenCounter.count(system_prompt, model) +
                TokenCounter.count(user_message, model)
            )
            ...
    ```

### Task 42.5: Add Truncation Events

- [ ] Emit observability event on truncation (1 file modification, ~20 LOC)
  - **Tests**: `tests/unit/test_context_truncation.py`
    - `test_emits_event_on_truncation()`
    - `test_event_includes_truncation_details()`
  - **Files**: `backend/llm/truncation.py`
  - **Implementation**:
    ```python
    @dataclass
    class TruncationEvent:
        original_tokens: int
        truncated_tokens: int
        history_messages_removed: int
        context_truncated: bool
        model: str
        timestamp: datetime

    class ContextTruncator:
        def __init__(self, config: TruncationConfig, event_callback: Optional[Callable] = None):
            self.config = config
            self.on_event = event_callback

        def truncate(self, ...) -> Tuple[str, List[Dict], bool]:
            ...
            if was_truncated and self.on_event:
                self.on_event(TruncationEvent(
                    original_tokens=original_total,
                    truncated_tokens=final_total,
                    history_messages_removed=len(history) - len(truncated_history),
                    context_truncated=context != original_context,
                    model=model,
                    timestamp=datetime.utcnow(),
                ))
            ...
    ```

### Task 42.6: Smart History Truncation

- [ ] Preserve system-critical history turns (1 file modification, ~25 LOC)
  - **Tests**: `tests/unit/test_context_truncation.py`
    - `test_preserves_first_user_message()`
    - `test_preserves_marked_important_messages()`
    - `test_truncates_middle_history()`
  - **Files**: `backend/llm/truncation.py`
  - **Implementation**:
    ```python
    def truncate(
        self,
        system_prompt: str,
        user_message: str,
        context: Optional[str],
        history: List[Dict],
        model: str,
    ) -> Tuple[str, List[Dict], bool]:
        ...
        # Keep first and last N turns, truncate middle
        if len(history) > 4:
            # Always keep first 2 messages (conversation setup)
            # Always keep last 2 messages (recent context)
            # Truncate from middle
            preserved_start = history[:2]
            preserved_end = history[-2:]
            middle = history[2:-2]

            truncated_history = preserved_start.copy()
            remaining = available - fixed_tokens - context_tokens
            remaining -= sum(TokenCounter.count(m.get("content", ""), model) + 4 for m in preserved_start)
            remaining -= sum(TokenCounter.count(m.get("content", ""), model) + 4 for m in preserved_end)

            # Add middle messages newest-first until exhausted
            for msg in reversed(middle):
                msg_tokens = TokenCounter.count(msg.get("content", ""), model) + 4
                if remaining >= msg_tokens:
                    truncated_history.append(msg)
                    remaining -= msg_tokens

            truncated_history.extend(preserved_end)
            was_truncated = len(truncated_history) < len(history)
        ...
    ```

---

## Impact on Roadmap

- **P30, P31**: Can now use accurate context limits per model.
- **Observability**: TruncationEvent should be stored/streamed like other events.
- **Performance**: Token counting overhead acceptable (~5ms) for typical requests.
- **Testing**: Need test fixtures with long context to verify truncation.
