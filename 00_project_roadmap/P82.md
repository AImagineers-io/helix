# P82: Improvement Plan for P76.md

## What's Weak / Missing

### Multi-Instance Deployment Gap
- In-memory pub/sub specified but no multi-instance strategy
- Helix runs on single server today but architecture doc mentions "Kubernetes-ready"
- No Redis upgrade path implementation details

### Connection Management Gaps
- No maximum concurrent SSE connections limit
- Missing backpressure handling for slow consumers
- No specification for buffer overflow behavior
- Connection cleanup on server restart undefined

### Protocol Gaps
- No reconnection protocol for clients (Last-Event-ID)
- Missing message ordering guarantees
- No specification for event ID format
- Heartbeat format unspecified

### Security Gaps
- "Auth required" mentioned but no implementation task
- No authorization granularity (can all API key holders see all events?)
- Missing rate limiting for SSE connections
- No consideration of connection exhaustion attacks

### Scope Contradiction
- "Historical event replay" excluded but "since" timestamp parameter included
- These are the same capability - contradiction needs resolution

## Why This Matters

SSE connections are long-lived and resource-intensive. Without connection limits, a single bad actor exhausts server resources. Without reconnection protocol, clients miss events during network blips. Without multi-instance strategy, horizontal scaling breaks the event stream. In-memory pub/sub loses all events on server restart.

## Proposed Improvements

### 1. Add Connection Limits

Add to Task 4 (Connection timeout handling):
```python
# Hard limits
MAX_CONCURRENT_CONNECTIONS = 100  # Per server instance
MAX_CONNECTIONS_PER_IP = 5        # Prevent single-client exhaustion
BUFFER_SIZE_PER_SUBSCRIBER = 1000  # Events, then drop oldest
```

Add success criteria:
- [ ] Connection rejected with 503 when at MAX_CONCURRENT_CONNECTIONS
- [ ] Connection rejected with 429 when IP exceeds MAX_CONNECTIONS_PER_IP
- [ ] Oldest events dropped when buffer exceeds 1000 (logged as warning)

### 2. Define Reconnection Protocol

Add new task:
- [ ] SSE reconnection support (2 files, ~60 LOC)
  - Tests: tests/integration/test_sse_reconnection.py
  - Files: api/routers/observability.py, services/event_stream_service.py
  - Implement Last-Event-ID header support
  - Event IDs: monotonic counter per connection session
  - On reconnect with Last-Event-ID: replay from buffer if available, else start fresh

Add success criteria:
- [ ] Reconnecting client with Last-Event-ID receives missed events (if in buffer)
- [ ] Reconnecting client without Last-Event-ID starts fresh (no duplicates)
- [ ] Event ID monotonically increases within connection

### 3. Resolve Historical Replay Contradiction

Replace confusing scope statement:

**Current (contradictory)**:
> - Excluded: Dashboard UI components, historical event replay
> - Query params: event_type (filter), since (timestamp)

**Corrected**:
```markdown
**Scope**:
- Included: Real-time event streaming from connection time forward
- Included: Limited replay from in-memory buffer (last 1000 events)
- Excluded: Persistent historical replay (query events from hours/days ago)

Note: "since" parameter filters buffer only, not database. For historical
analysis, query ObservabilityEvent table directly via analytics API.
```

### 4. Add Authorization Granularity

Add new task:
- [ ] Event stream authorization (2 files, ~50 LOC)
  - Tests: tests/integration/test_event_stream_auth.py
  - Files: api/routers/observability.py, core/auth.py
  - Permission levels:
    - `events:read:all` - See all events (admin)
    - `events:read:own` - See events for own conversations only
  - Default API key gets `events:read:all` (single-tenant deployment)

Add success criterion:
- [ ] API key without events permission receives 403
- [ ] Invalid API key receives 401

### 5. Define SSE Message Format

Add explicit format specification:
```
event: <event_type>
id: <monotonic_id>
data: {"schema_version":1,"correlation_id":"...","payload":{...}}

:heartbeat

event: error
data: {"error":"buffer_overflow","dropped_count":50}
```

Add success criteria:
- [ ] All messages follow SSE spec (event/id/data fields)
- [ ] Heartbeat sent as SSE comment (`:heartbeat\n\n`)
- [ ] Buffer overflow communicated via error event

### 6. Add Redis Upgrade Path

Document in implementation notes (not new task):
```markdown
### Redis Upgrade Path (Future P-number)

When Helix scales to multiple instances:
1. Replace in-memory dict with Redis pub/sub
2. Use Redis Streams for buffer persistence
3. Consumer group per SSE connection
4. No API changes required - same interface

**Trigger**: When deploying >1 backend instance
```

Add to Task 1:
```python
# Interface must support Redis swap
class EventStreamService(Protocol):
    async def subscribe(self, subscriber_id: str) -> AsyncIterator[Event]: ...
    async def publish(self, event: Event) -> None: ...

# Current: InMemoryEventStreamService
# Future: RedisEventStreamService
```

### 7. Add Rate Limiting

Add new task:
- [ ] SSE connection rate limiting (1 file, ~40 LOC)
  - Tests: tests/integration/test_sse_rate_limit.py
  - Files: core/middleware.py
  - Limit: 10 new connections per minute per IP
  - Applies to connection initiation only (not data flow)

Add success criterion:
- [ ] 11th connection attempt within 1 minute returns 429

### 8. Add Graceful Shutdown

Add to Task 4:
```python
# On server shutdown signal (SIGTERM):
# 1. Stop accepting new connections
# 2. Send "server_shutdown" event to all subscribers
# 3. Close all connections gracefully
# 4. Wait up to 5s for cleanup
```

Add success criterion:
- [ ] Server shutdown sends notification to all connected clients
- [ ] Clients receive clean close (not abrupt disconnect)

## Impact on Roadmap

- **P75** must emit events with ID field for Last-Event-ID support
- **P78** dashboard EventStreamPanel must implement reconnection logic
- Add P87: "Redis Event Stream" - implement when multi-instance deployment needed
- Auth improvements may require P88: "Granular Permissions System"
