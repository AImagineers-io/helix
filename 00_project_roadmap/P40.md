# P40: Improvement Plan for P33.md

## What's Weak / Missing

- **Cache key doesn't include model name**: Same prompt with different models should not share cache. gpt-4o-mini and gpt-4o produce different responses.
- **Cache key doesn't include temperature/max_tokens**: Different parameters produce different outputs.
- **`_graceful_fallback` method undefined**: `LLMService.generate` references it but no implementation.
- **Redis KEYS command used in invalidation**: KEYS blocks Redis on large datasets. Use SCAN instead.
- **Metrics not persisted**: In-memory `CacheMetrics` lost on restart. No visibility into historical hit rates.
- **No Redis connection failure handling**: Redis down should not crash LLM service.
- **No semantic similarity cache**: Only exact matches hit cache. Similar queries miss.
- **Cache serialization uses `__dict__`**: Breaks if LLMResponse has non-serializable fields (like datetime).

---

## Why This Matters

Wrong cache key causes cache poisoning (returning gpt-4o-mini response when gpt-4o requested). KEYS on 100k+ keys blocks Redis for seconds. Redis failures should degrade to no-cache, not crash. Lost metrics hide performance problems.

---

## Proposed Improvements

### Task 40.1: Include All Parameters in Cache Key

- [ ] Add model, temperature, max_tokens to key (1 file modification, ~15 LOC)
  - **Tests**: `tests/unit/test_response_cache.py`
    - `test_different_models_different_keys()`
    - `test_different_temperatures_different_keys()`
    - `test_different_max_tokens_different_keys()`
  - **Files**: `backend/llm/cache.py`
  - **Implementation**:
    ```python
    def _generate_key(
        self,
        system_prompt: str,
        user_message: str,
        context: Optional[str],
        model: str = "gpt-4o-mini",
        temperature: float = 0.7,
        max_tokens: int = 1024,
    ) -> str:
        content = json.dumps({
            "system": system_prompt,
            "user": user_message,
            "context": context or "",
            "model": model,
            "temperature": round(temperature, 2),  # Avoid float precision issues
            "max_tokens": max_tokens,
        }, sort_keys=True)
        hash_value = hashlib.sha256(content.encode()).hexdigest()[:16]
        return f"{self.config.prefix}:{hash_value}"
    ```

### Task 40.2: Use SCAN Instead of KEYS

- [ ] Replace KEYS with SCAN for invalidation (1 file modification, ~20 LOC)
  - **Tests**: `tests/unit/test_response_cache.py`
    - `test_invalidation_uses_scan()`
    - `test_invalidation_handles_large_keyspace()`
  - **Files**: `backend/llm/cache.py`
  - **Implementation**:
    ```python
    async def invalidate_all(self) -> int:
        """Clear entire response cache using SCAN (non-blocking)."""
        pattern = f"{self.config.prefix}:*"
        deleted = 0
        cursor = 0
        while True:
            cursor, keys = await self.redis.scan(cursor, match=pattern, count=100)
            if keys:
                deleted += await self.redis.delete(*keys)
            if cursor == 0:
                break
        return deleted
    ```

### Task 40.3: Handle Redis Connection Failures

- [ ] Gracefully degrade when Redis unavailable (1 file modification, ~25 LOC)
  - **Tests**: `tests/unit/test_response_cache.py`
    - `test_get_returns_none_on_redis_error()`
    - `test_set_silently_fails_on_redis_error()`
    - `test_logs_redis_errors()`
  - **Files**: `backend/llm/cache.py`
  - **Implementation**:
    ```python
    from redis.exceptions import RedisError

    async def get(self, ...) -> Optional[LLMResponse]:
        if not self.config.enabled:
            return None
        try:
            key = self._generate_key(...)
            data = await self.redis.get(key)
            if data:
                self.metrics.hits += 1
                return self._deserialize(data)
            self.metrics.misses += 1
            return None
        except RedisError as e:
            logger.warning(f"Redis error on cache get: {e}")
            self.metrics.misses += 1
            return None

    async def set(self, ...) -> None:
        if not self.config.enabled:
            return
        try:
            key = self._generate_key(...)
            data = self._serialize(response)
            await self.redis.setex(key, self.config.ttl_seconds, data)
        except RedisError as e:
            logger.warning(f"Redis error on cache set: {e}")
    ```

### Task 40.4: Use Proper Serialization

- [ ] Add explicit serialize/deserialize methods (1 file modification, ~20 LOC)
  - **Tests**: `tests/unit/test_response_cache.py`
    - `test_serialization_handles_all_fields()`
    - `test_deserialization_reconstructs_response()`
  - **Files**: `backend/llm/cache.py`
  - **Implementation**:
    ```python
    def _serialize(self, response: LLMResponse) -> str:
        return json.dumps({
            "content": response.content,
            "model": response.model,
            "input_tokens": response.input_tokens,
            "output_tokens": response.output_tokens,
            "provider": response.provider,
            "request_id": response.request_id,
            "latency_ms": response.latency_ms,
        })

    def _deserialize(self, data: str) -> LLMResponse:
        obj = json.loads(data)
        return LLMResponse(
            content=obj["content"],
            model=obj["model"],
            input_tokens=obj["input_tokens"],
            output_tokens=obj["output_tokens"],
            provider=obj["provider"],
            request_id=obj.get("request_id", str(uuid4())),
            latency_ms=obj.get("latency_ms", 0.0),
            cached=True,  # Mark as cached on deserialize
        )
    ```

### Task 40.5: Persist Cache Metrics to Redis

- [ ] Store hit/miss counters in Redis (1 file modification, ~30 LOC)
  - **Tests**: `tests/unit/test_response_cache.py`
    - `test_metrics_persisted_to_redis()`
    - `test_metrics_survive_restart()`
    - `test_get_metrics_returns_totals()`
  - **Files**: `backend/llm/cache.py`
  - **Implementation**:
    ```python
    class ResponseCache:
        METRICS_KEY = "llm:cache:metrics"

        async def _increment_metric(self, field: str):
            try:
                await self.redis.hincrby(self.METRICS_KEY, field, 1)
            except RedisError:
                pass  # Best effort

        async def get_metrics(self) -> CacheMetrics:
            try:
                data = await self.redis.hgetall(self.METRICS_KEY)
                return CacheMetrics(
                    hits=int(data.get(b"hits", 0)),
                    misses=int(data.get(b"misses", 0)),
                )
            except RedisError:
                return CacheMetrics()

        async def get(self, ...) -> Optional[LLMResponse]:
            ...
            if data:
                await self._increment_metric("hits")
                ...
            await self._increment_metric("misses")
            ...
    ```

### Task 40.6: Define _graceful_fallback Method

- [ ] Implement graceful fallback in LLMService (1 file modification, ~20 LOC)
  - **Tests**: `tests/unit/test_cached_llm_service.py`
    - `test_graceful_fallback_returns_apology()`
    - `test_graceful_fallback_logs_failure()`
  - **Files**: `backend/llm/service.py`
  - **Implementation**:
    ```python
    async def _graceful_fallback(self, user_message: str) -> LLMResponse:
        """Return apologetic response when all providers and cache fail."""
        logger.error(f"All fallbacks exhausted for message: {user_message[:100]}...")
        return LLMResponse(
            content="I'm sorry, I'm having trouble processing your request right now. Please try again in a moment.",
            model="fallback",
            input_tokens=0,
            output_tokens=0,
            provider="graceful_fallback",
        )
    ```

---

## Impact on Roadmap

- **API signature change**: `cache.get()` and `cache.set()` now require model, temperature, max_tokens parameters.
- **P33 callers**: Must pass additional parameters to cache methods.
- **Monitoring**: Redis-persisted metrics enable dashboard without additional storage.
- **Dependencies**: Import `RedisError` from redis.exceptions.
