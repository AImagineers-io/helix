# P67: Improvement Plan for P60.md

## Target Phase

P60: File Parsers

---

## What's Weak / Missing

- **No streaming support**: All parsers call `file_content.decode()` loading entire file into memory; 10MB file = 10MB+ memory spike
- **Missing delimiter flexibility**: CSV parser assumes comma delimiter; European CSVs use semicolon, Excel exports sometimes use tab
- **No BOM handling**: UTF-8 BOM (`\xef\xbb\xbf`) at file start will corrupt first column header detection
- **Excel format omitted**: Clients commonly export from Excel as `.xlsx`; forcing CSV export adds friction
- **No progress callback**: Large file parsing provides no feedback; UI shows no progress during parse phase
- **Column detection case sensitivity unclear**: Does "Question" match the header set? Inconsistent behavior risk
- **No row limit enforcement**: Parser will attempt to parse 100k-row file even if system can't handle it
- **Text parser state machine implicit**: Complex Q:/A: pattern matching has no explicit state diagram; maintenance risk
- **Missing content type validation**: Extension-based detection can be spoofed; malicious files bypass checks
- **No partial parse option**: Preview requires full parse; slow for large files

---

## Why This Matters

- **Memory exhaustion**: Without streaming, importing a large file on the apps01 VM (8GB RAM shared across services) can OOM the container
- **User friction**: Forcing CSV format excludes non-technical users who work natively in Excel
- **Silent data corruption**: BOM bytes misread as column name causes "question" column to not match, entire file rejected
- **Poor UX for large files**: No progress during parsing makes the system appear frozen
- **Security exposure**: Extension spoofing allows uploading executable content disguised as CSV

---

## Proposed Improvements

### 1. Add Streaming Parser Interface

```python
from typing import Iterator, Callable

class StreamingParser(ABC):
    @abstractmethod
    def parse_stream(
        self,
        file_content: bytes,
        on_row: Callable[[ParsedQA], None],
        on_progress: Callable[[int], None] = None,  # bytes processed
    ) -> int:
        """Stream-parse file, calling on_row for each QA pair. Returns total count."""

class CSVParser(StreamingParser):
    def parse_stream(self, file_content: bytes, on_row, on_progress=None):
        # Use csv.reader with chunked StringIO
        # Call on_progress every 1000 rows or 100KB
```

Add tests:
- `test_streaming_parser_calls_on_row_per_item()`
- `test_streaming_parser_reports_progress()`
- `test_streaming_parser_handles_10mb_file()`

### 2. Support Configurable CSV Delimiters

```python
class CSVParser:
    SUPPORTED_DELIMITERS = [",", ";", "\t", "|"]

    def __init__(self, delimiter: str = None):
        self.delimiter = delimiter  # None = auto-detect

    def _detect_delimiter(self, sample: str) -> str:
        """Detect delimiter from first 5 lines using csv.Sniffer."""
        try:
            dialect = csv.Sniffer().sniff(sample, delimiters=",;\t|")
            return dialect.delimiter
        except csv.Error:
            return ","  # fallback
```

Add tests:
- `test_parses_semicolon_csv()`
- `test_parses_tab_separated()`
- `test_auto_detects_delimiter()`

### 3. Handle BOM Characters

```python
def _remove_bom(self, content: bytes) -> bytes:
    """Strip UTF-8, UTF-16 LE/BE BOMs."""
    boms = [
        (b'\xef\xbb\xbf', 'utf-8'),
        (b'\xff\xfe', 'utf-16-le'),
        (b'\xfe\xff', 'utf-16-be'),
    ]
    for bom, encoding in boms:
        if content.startswith(bom):
            return content[len(bom):]
    return content
```

Add test: `test_handles_utf8_bom()`

### 4. Add Excel Parser

```python
# New file: backend/parsers/excel_parser.py
# Dependency: openpyxl (add to requirements.txt)

class ExcelParser(StreamingParser):
    def parse_stream(self, file_content: bytes, on_row, on_progress=None):
        workbook = openpyxl.load_workbook(io.BytesIO(file_content), read_only=True)
        sheet = workbook.active
        headers = [cell.value.lower().strip() if cell.value else "" for cell in next(sheet.iter_rows())]
        q_col, a_col = self._detect_columns(headers)

        for row_num, row in enumerate(sheet.iter_rows(min_row=2), start=2):
            # Extract values, call on_row
```

Add tests:
- `test_parses_xlsx_file()`
- `test_detects_columns_in_excel()`
- `test_handles_multiple_sheets()` - uses first sheet

Update parser factory:
```python
elif ext in {".xlsx", ".xls"}:
    return ExcelParser()
```

### 5. Enforce Row Limits

```python
class ParserConfig:
    MAX_ROWS = 10000  # Configurable via env
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB

class CSVParser:
    def __init__(self, config: ParserConfig = None):
        self.config = config or ParserConfig()

    def parse_stream(self, ...):
        row_count = 0
        for row in reader:
            row_count += 1
            if row_count > self.config.MAX_ROWS:
                raise ParseError(row_count, f"File exceeds maximum {self.config.MAX_ROWS} rows")
            on_row(parsed)
```

Add test: `test_raises_on_exceeding_row_limit()`

### 6. Normalize Header Detection

```python
QUESTION_HEADERS = {"question", "q", "query", "faq", "ask"}  # All lowercase

def _detect_columns(self, headers: List[str]) -> Tuple[int, int]:
    normalized = [h.lower().strip() if h else "" for h in headers]
    q_col = next((i for i, h in enumerate(normalized) if h in QUESTION_HEADERS), None)
    # ...
```

Add test: `test_column_detection_case_insensitive()`

### 7. Add Content Type Validation

```python
import magic  # python-magic library

def validate_content_type(filename: str, content: bytes) -> str:
    """Validate file content matches extension. Returns detected type."""
    detected = magic.from_buffer(content[:2048], mime=True)
    ext = Path(filename).suffix.lower()

    ALLOWED_TYPES = {
        ".csv": ["text/csv", "text/plain", "application/csv"],
        ".json": ["application/json", "text/plain"],
        ".txt": ["text/plain"],
        ".xlsx": ["application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"],
    }

    if ext not in ALLOWED_TYPES:
        raise ParseError(0, f"Unsupported file type: {ext}")
    if detected not in ALLOWED_TYPES[ext]:
        raise ParseError(0, f"File content ({detected}) doesn't match extension ({ext})")
    return detected
```

Add test: `test_rejects_mismatched_content_type()`

### 8. Add Quick Row Count Estimation

```python
def estimate_row_count(content: bytes, file_type: str) -> int:
    """Quick estimate without full parse for progress UI."""
    if file_type == "csv":
        return content.count(b'\n')
    elif file_type == "json":
        # Count top-level array elements by counting '},{' patterns
        return content.count(b'},{') + 1
    elif file_type == "text":
        return content.count(b'Q:') + content.count(b'Question:')
    return 0
```

---

## Impact on Roadmap

| Affected Phase | Change |
|----------------|--------|
| P59 | Add `estimated_rows` field for pre-parse count |
| P62 | Batch processor receives streaming iterator instead of list |
| P62 | Import service must handle progress from both parse and process phases |
| P63 | Add `.xlsx` to accepted extensions |
| P64 | Progress component shows parse progress before process progress |
| P65 | Add streaming parser tests, Excel file fixtures |

---

## New Tasks for P60

- **Task 60.6**: Implement streaming parser interface with progress callbacks
- **Task 60.7**: Add delimiter auto-detection and configuration
- **Task 60.8**: Implement BOM stripping for all encodings
- **Task 60.9**: Create Excel parser with openpyxl
- **Task 60.10**: Add content type validation using magic bytes
- **Task 60.11**: Implement row limit enforcement
- **Task 60.12**: Add quick row count estimation function

---

## Dependency Changes

Add to `backend/requirements.txt`:
```
openpyxl>=3.1.0
python-magic>=0.4.27
```
