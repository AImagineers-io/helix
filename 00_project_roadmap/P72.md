# P72: Improvement Plan for P65.md

## Target Phase

P65: Import System Integration Tests

---

## What's Weak / Missing

- **Performance baseline arbitrary**: "60 seconds for 1000 rows" has no empirical basis; could be too slow or unrealistically fast
- **"Without embeddings" caveat undermines validity**: Real imports include embeddings; test doesn't measure actual performance
- **No stress tests**: 1000 rows tested, but what about 10k? 50k? No upper bound validation
- **No chaos engineering**: DB failure, network timeout, API rate limit not tested
- **Missing concurrent import race conditions**: `test_concurrent_imports_isolated()` exists but doesn't test actual race conditions
- **No security tests**: SQL injection, XSS via import content, path traversal not tested
- **Static fixtures only**: No property-based testing for parser edge cases
- **No CI/CD integration strategy**: Slow tests not tagged, will block PR pipelines
- **Missing test cleanup**: Generated QA pairs from integration tests pollute test database
- **No alerting/monitoring verification**: Metrics and events emitted but not asserted
- **Memory tests vague**: "Memory stable" not quantified; no actual measurement
- **No rollback scenario tests**: What if import needs to be undone?

---

## Why This Matters

- **False confidence**: Tests pass but production import takes 5 minutes because embeddings not included
- **Production surprises**: 10k-row import never tested; first real large import crashes
- **Security gaps**: Imported content with XSS reaches admin UI; discovered by attacker, not tests
- **CI/CD friction**: Slow tests run on every PR; developer velocity suffers
- **Data pollution**: Test QA pairs left in database; interfere with other tests or manual testing

---

## Proposed Improvements

### 1. Establish Performance Baselines with Embeddings

```python
# tests/integration/test_import_performance.py

@pytest.fixture
def performance_baseline():
    """Load baseline expectations from config."""
    return {
        "100_rows_with_embeddings": 30,     # seconds
        "1000_rows_with_embeddings": 300,   # seconds (5 min)
        "1000_rows_without_embeddings": 30, # seconds
        "memory_per_1000_rows_mb": 100,     # max memory growth
    }

@pytest.mark.slow
@pytest.mark.embeddings
async def test_import_1000_rows_with_embeddings(
    import_service,
    test_csv_1000_rows,
    performance_baseline,
    mock_embedding_service,  # Use mock with realistic latency
):
    """Test real-world import performance including embedding generation."""
    # Configure mock to simulate real embedding latency
    mock_embedding_service.set_latency(0.1)  # 100ms per batch

    start = time.time()
    job_id = await import_service.start_import(
        filename="test_1000.csv",
        content=test_csv_1000_rows,
        source_type="csv"
    )

    # Wait for completion
    job = await wait_for_job_completion(job_id, timeout=performance_baseline["1000_rows_with_embeddings"])

    duration = time.time() - start
    assert job.status == ImportJobStatus.COMPLETED
    assert duration < performance_baseline["1000_rows_with_embeddings"], \
        f"Import took {duration}s, expected under {performance_baseline['1000_rows_with_embeddings']}s"

    # Verify all embeddings generated
    pairs = session.query(QAPair).filter(QAPair.import_job_id == job_id).all()
    assert all(pair.embedding_status == "completed" for pair in pairs)
```

### 2. Add Stress Tests

```python
# tests/integration/test_import_stress.py

@pytest.mark.stress
@pytest.mark.slow
class TestImportStress:

    @pytest.mark.parametrize("row_count", [5000, 10000])
    async def test_large_file_import(self, import_service, row_count):
        """Test import handles large files without failure."""
        content = generate_csv_content(row_count)

        job_id = await import_service.start_import(
            filename=f"stress_{row_count}.csv",
            content=content,
            source_type="csv"
        )

        # Allow 1 second per 100 rows
        timeout = (row_count / 100) * 1
        job = await wait_for_job_completion(job_id, timeout=timeout)

        assert job.status == ImportJobStatus.COMPLETED
        assert job.success_count >= row_count * 0.99  # Allow 1% failures

    async def test_import_reaches_row_limit(self, import_service):
        """Test system enforces row limits gracefully."""
        # Generate file exceeding limit
        content = generate_csv_content(15000)  # Above 10k limit

        with pytest.raises(HTTPException) as exc:
            await import_service.start_import(
                filename="oversized.csv",
                content=content,
                source_type="csv"
            )

        assert exc.value.status_code == 400
        assert "exceeds maximum" in str(exc.value.detail)
```

### 3. Add Chaos Engineering Tests

```python
# tests/integration/test_import_chaos.py

@pytest.mark.chaos
class TestImportChaos:

    async def test_db_disconnect_mid_import(
        self,
        import_service,
        test_csv_100_rows,
        db_proxy,  # Proxy that can simulate disconnects
    ):
        """Test import recovers from DB disconnect."""
        job_id = await import_service.start_import(
            filename="test.csv",
            content=test_csv_100_rows,
            source_type="csv"
        )

        # Wait until 50% progress, then disconnect DB
        await wait_for_progress(job_id, percent=50)
        db_proxy.disconnect()
        await asyncio.sleep(2)
        db_proxy.reconnect()

        # Job should eventually recover via retry
        job = await wait_for_job_completion(job_id, timeout=120)
        assert job.status == ImportJobStatus.COMPLETED
        assert job.success_count > 0

    async def test_embedding_api_rate_limit(
        self,
        import_service,
        test_csv_500_rows,
        mock_embedding_service,
    ):
        """Test import handles embedding API rate limits."""
        # Configure mock to return 429 for first 10 requests
        mock_embedding_service.set_rate_limit_errors(10)

        job_id = await import_service.start_import(
            filename="test.csv",
            content=test_csv_500_rows,
            source_type="csv"
        )

        job = await wait_for_job_completion(job_id, timeout=300)

        # Should complete despite rate limits (with retries)
        assert job.status == ImportJobStatus.COMPLETED
        assert job.success_count == 500

    async def test_server_crash_recovery(
        self,
        import_service,
        test_csv_100_rows,
    ):
        """Test stale job recovery after simulated crash."""
        job_id = await import_service.start_import(
            filename="test.csv",
            content=test_csv_100_rows,
            source_type="csv"
        )

        # Wait until processing starts
        await wait_for_status(job_id, ImportJobStatus.PROCESSING)

        # Simulate crash: mark job as stale by updating timestamp
        session.execute(
            update(ImportJob)
            .where(ImportJob.id == job_id)
            .values(updated_at=datetime.utcnow() - timedelta(hours=1))
        )

        # Run recovery
        recovered = await import_service.recover_stale_jobs(timeout_minutes=30)
        assert job_id in recovered
```

### 4. Add Security Tests

```python
# tests/integration/test_import_security.py

@pytest.mark.security
class TestImportSecurity:

    @pytest.mark.parametrize("malicious_content", [
        '<script>alert("xss")</script>',
        '"><img src=x onerror=alert(1)>',
        "javascript:alert('xss')",
        "'; DROP TABLE qa_pair; --",
    ])
    async def test_xss_sanitized(
        self,
        import_service,
        malicious_content,
    ):
        """Test XSS payloads are sanitized in imported content."""
        csv = f'question,answer\n"What is this?","{malicious_content}"'

        job_id = await import_service.start_import(
            filename="xss_test.csv",
            content=csv.encode(),
            source_type="csv"
        )

        job = await wait_for_job_completion(job_id)
        assert job.status == ImportJobStatus.COMPLETED

        # Verify content was sanitized
        pair = session.query(QAPair).filter(QAPair.import_job_id == job_id).first()
        assert "<script>" not in pair.answer
        assert "javascript:" not in pair.answer
        assert "DROP TABLE" not in pair.answer

    async def test_path_traversal_in_filename(
        self,
        client,
        test_csv_content,
    ):
        """Test path traversal in filename is sanitized."""
        response = await client.post(
            "/qa/import/csv",
            files={"file": ("../../../etc/passwd", test_csv_content)},
        )

        assert response.status_code == 202
        job = await get_job(response.json()["job_id"])
        # Filename should be sanitized
        assert ".." not in job.filename
        assert "/" not in job.filename

    async def test_zip_bomb_rejected(self, client):
        """Test compressed files that expand dangerously are rejected."""
        # Create a small file that decompresses to huge size
        zip_bomb = create_zip_bomb(compressed_size=1024, decompressed_size=1024*1024*100)

        response = await client.post(
            "/qa/import/csv",
            files={"file": ("bomb.csv.gz", zip_bomb)},
        )

        assert response.status_code == 400
```

### 5. Add Property-Based Tests

```python
# tests/integration/test_parser_properties.py

from hypothesis import given, strategies as st, settings

@given(
    questions=st.lists(
        st.text(min_size=1, max_size=1000),
        min_size=1,
        max_size=100
    ),
    answers=st.lists(
        st.text(min_size=1, max_size=5000),
        min_size=1,
        max_size=100
    )
)
@settings(max_examples=100, deadline=30000)
def test_csv_parser_roundtrip(questions, answers):
    """Test CSV parser handles arbitrary valid content."""
    # Create CSV with random content
    rows = list(zip(questions, answers))
    csv_content = "question,answer\n"
    for q, a in rows:
        # Proper CSV escaping
        q_escaped = q.replace('"', '""')
        a_escaped = a.replace('"', '""')
        csv_content += f'"{q_escaped}","{a_escaped}"\n'

    # Parse should not crash
    parser = CSVParser()
    try:
        parsed = parser.parse(csv_content.encode())
        assert len(parsed) == len(rows)
    except ParseError as e:
        # ParseError is acceptable for invalid content
        pass

@given(
    encoding=st.sampled_from(["utf-8", "latin-1", "cp1252", "utf-16"]),
    text=st.text(alphabet=st.characters(blacklist_categories=("Cs",)), min_size=10)
)
def test_encoding_detection(encoding, text):
    """Test parser handles various encodings."""
    try:
        content = text.encode(encoding)
        detected = detect_encoding(content)
        # Should detect or fallback to something valid
        assert detected in ["utf-8", "latin-1", "cp1252", "utf-16", "ascii"]
    except UnicodeEncodeError:
        pass  # Some text can't encode to all encodings
```

### 6. Add CI/CD Integration

```python
# tests/conftest.py

def pytest_configure(config):
    config.addinivalue_line("markers", "slow: marks tests as slow (deselect with '-m \"not slow\"')")
    config.addinivalue_line("markers", "stress: marks tests as stress tests")
    config.addinivalue_line("markers", "chaos: marks tests as chaos engineering tests")
    config.addinivalue_line("markers", "security: marks tests as security tests")
    config.addinivalue_line("markers", "embeddings: marks tests requiring embedding service")

# pytest.ini
[pytest]
markers =
    slow: marks tests as slow (run with --slow flag)
    stress: stress tests (run nightly only)
    chaos: chaos engineering tests (run nightly only)
    security: security tests (run on every PR)
    embeddings: tests requiring embedding service

# CI configuration note:
# - PR pipeline: pytest -m "not slow and not stress and not chaos"
# - Nightly pipeline: pytest --slow --stress --chaos
```

### 7. Add Test Cleanup

```python
# tests/integration/conftest.py

@pytest.fixture(autouse=True)
async def cleanup_import_test_data(session):
    """Clean up QA pairs created during import tests."""
    # Record IDs before test
    before_ids = set(
        session.execute(select(QAPair.id)).scalars().all()
    )

    yield

    # Delete new pairs after test
    after_ids = set(
        session.execute(select(QAPair.id)).scalars().all()
    )
    new_ids = after_ids - before_ids

    if new_ids:
        session.execute(
            delete(QAPair).where(QAPair.id.in_(new_ids))
        )
        session.commit()

@pytest.fixture(autouse=True)
async def cleanup_import_jobs(session):
    """Clean up import jobs created during tests."""
    yield

    # Delete all test import jobs
    session.execute(
        delete(ImportJob).where(ImportJob.filename.like("test%"))
    )
    session.commit()
```

### 8. Add Monitoring Verification Tests

```python
# tests/integration/test_import_observability.py

class TestImportObservability:

    async def test_status_change_emits_event(
        self,
        import_service,
        test_csv_10_rows,
        event_capture,  # Fixture that captures ObservabilityEvents
    ):
        """Test import status changes emit observability events."""
        job_id = await import_service.start_import(
            filename="test.csv",
            content=test_csv_10_rows,
            source_type="csv"
        )

        await wait_for_job_completion(job_id)

        events = event_capture.get_events(job_id=str(job_id))
        event_types = [e.event_type for e in events]

        assert "import_job_status_changed" in event_types
        # Check transitions: PENDING -> PROCESSING -> COMPLETED
        status_events = [e for e in events if e.event_type == "import_job_status_changed"]
        assert len(status_events) >= 2

    async def test_metrics_recorded(
        self,
        import_service,
        test_csv_100_rows,
        prometheus_registry,
    ):
        """Test import records Prometheus metrics."""
        job_id = await import_service.start_import(
            filename="test.csv",
            content=test_csv_100_rows,
            source_type="csv"
        )

        await wait_for_job_completion(job_id)

        # Check metrics
        batches_total = prometheus_registry.get_sample_value(
            "helix_import_batches_total",
            {"status": "success"}
        )
        assert batches_total > 0
```

### 9. Add Memory Measurement Tests

```python
# tests/integration/test_import_memory.py

import tracemalloc

@pytest.mark.slow
async def test_memory_growth_bounded(import_service, performance_baseline):
    """Test memory doesn't grow unbounded during large import."""
    tracemalloc.start()

    initial_memory = tracemalloc.get_traced_memory()[0]

    # Import 1000 rows
    content = generate_csv_content(1000)
    job_id = await import_service.start_import(
        filename="memory_test.csv",
        content=content,
        source_type="csv"
    )

    await wait_for_job_completion(job_id, timeout=300)

    peak_memory = tracemalloc.get_traced_memory()[1]
    tracemalloc.stop()

    memory_growth_mb = (peak_memory - initial_memory) / (1024 * 1024)

    assert memory_growth_mb < performance_baseline["memory_per_1000_rows_mb"], \
        f"Memory grew by {memory_growth_mb}MB, expected under {performance_baseline['memory_per_1000_rows_mb']}MB"
```

### 10. Add Rollback Tests

```python
# tests/integration/test_import_rollback.py

class TestImportRollback:

    async def test_failed_import_cleans_up_partial_data(
        self,
        import_service,
        test_csv_with_error_at_row_50,
    ):
        """Test failed import cleans up partially created data."""
        initial_count = session.query(QAPair).count()

        job_id = await import_service.start_import(
            filename="test.csv",
            content=test_csv_with_error_at_row_50,
            source_type="csv"
        )

        job = await wait_for_job_completion(job_id)

        # Based on batch processing, partial data IS kept (that's the design)
        # But verify error count matches
        final_count = session.query(QAPair).count()
        assert final_count - initial_count == job.success_count

    async def test_cancelled_import_stops_processing(
        self,
        import_service,
        test_csv_1000_rows,
    ):
        """Test cancelled import stops adding new data."""
        job_id = await import_service.start_import(
            filename="test.csv",
            content=test_csv_1000_rows,
            source_type="csv"
        )

        # Wait until 20% processed, then cancel
        await wait_for_progress(job_id, percent=20)
        await import_service.cancel_import(job_id)

        # Wait a bit, verify no more progress
        await asyncio.sleep(5)

        job = job_repository.get_by_id(job_id)
        assert job.status == ImportJobStatus.CANCELLED
        assert job.processed_rows < 500  # Should have stopped early
```

---

## Impact on Roadmap

| Affected Phase | Change |
|----------------|--------|
| P59-P64 | Tests verify all improvements from P66-P71 |
| CI/CD | Add nightly test pipeline for slow/stress tests |
| Documentation | Add testing strategy to development guidelines |

---

## New Tasks for P65

- **Task 65.8**: Create performance baseline configuration with embedding-inclusive tests
- **Task 65.9**: Add stress tests for 5k and 10k row imports
- **Task 65.10**: Implement chaos engineering tests (DB disconnect, rate limits)
- **Task 65.11**: Add security tests (XSS, injection, path traversal)
- **Task 65.12**: Add property-based tests with Hypothesis
- **Task 65.13**: Configure pytest markers for CI/CD integration
- **Task 65.14**: Implement automatic test data cleanup fixtures
- **Task 65.15**: Add observability verification tests
- **Task 65.16**: Add memory measurement tests with tracemalloc
- **Task 65.17**: Add rollback and cancellation integration tests

---

## Dependency Changes

Add to `backend/requirements-dev.txt`:
```
hypothesis>=6.82.0      # Property-based testing
toxiproxy>=2.1.0       # Chaos engineering (DB proxy)
```

Update `pytest.ini`:
```ini
[pytest]
markers =
    slow: slow tests (deselect with -m "not slow")
    stress: stress tests (nightly only)
    chaos: chaos engineering tests (nightly only)
    security: security tests (every PR)
    embeddings: requires embedding service
```
