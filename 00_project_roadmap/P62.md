# P62: Batch Processing & Import Service

## 1. Objective

**What**: Build batch processor for large imports and import service that orchestrates the complete flow from parsing to embedding generation.

**Why**: Large imports (1000+ rows) require careful handling: batch commits for partial success, progress tracking for UI feedback, memory management, and efficient embedding API calls. The import service ties together parsing, validation, and batch processing.

**Scope**:
- Included: Batch processor, progress callbacks, import service orchestration, background task integration
- Excluded: Parsers (P60), validation (P61), API endpoints (P63), Admin UI (P64)

**Dependencies**: P59 (ImportJob model/repository), P60 (parsers), P61 (validation pipeline), P17a (EmbeddingService)

---

## 2. Implementation Tasks

### Task 62.1: Create Batch Processor

- [ ] Process imports in configurable batches (1 file, ~100 LOC)
  - **Tests**: `tests/unit/test_batch_processor.py`
    - `test_processes_in_batches()` - respects batch_size
    - `test_commits_after_each_batch()` - partial success preserved
    - `test_calls_progress_callback()` - reports after each batch
    - `test_continues_on_row_error()` - one bad row doesn't stop import
    - `test_tracks_success_count()` - counts successful inserts
    - `test_tracks_error_count()` - counts failed rows
    - `test_collects_error_details()` - stores row + message
    - `test_handles_empty_batch()` - no errors on empty input
    - `test_configurable_batch_size()` - default 50, configurable
  - **Files**: `backend/services/batch_processor.py`
  - **Implementation**:
    ```python
    @dataclass
    class BatchResult:
        success_count: int
        error_count: int
        errors: List[dict]  # {row, field, message}
        created_ids: List[UUID]

    class BatchProcessor:
        def __init__(self, session: Session, batch_size: int = 50):
            self.session = session
            self.batch_size = batch_size

        def process(
            self,
            items: List[ParsedQA],
            on_progress: Callable[[int, int], None] = None
        ) -> BatchResult:
            """Process items in batches, committing each batch."""
    ```

### Task 62.2: Create QA Pair Creator

- [ ] Create QA pairs from parsed data with embedding trigger (1 file, ~60 LOC)
  - **Tests**: `tests/unit/test_qa_creator.py`
    - `test_creates_qa_pair()` - inserts into database
    - `test_sets_status_draft()` - default status
    - `test_sets_category()` - from parsed data
    - `test_sets_tags()` - from parsed data
    - `test_sanitizes_input()` - strips HTML/whitespace
    - `test_queues_embedding()` - marks for embedding generation
  - **Files**: `backend/services/qa_creator.py`
  - **Implementation**:
    ```python
    class QACreator:
        def __init__(self, session: Session, qa_service: QAService):
            self.session = session
            self.qa_service = qa_service

        def create_from_parsed(self, parsed: ParsedQA) -> QAPair:
            """Create QAPair from parsed import data."""
    ```

### Task 62.3: Create Import Service

- [ ] Orchestrate complete import flow (2 files, ~150 LOC)
  - **Tests**: `tests/unit/test_import_service.py`
    - `test_creates_import_job()` - job created with PENDING
    - `test_sets_job_processing()` - status changes on start
    - `test_parses_file()` - calls appropriate parser
    - `test_validates_parsed_data()` - runs validation pipeline
    - `test_processes_in_batches()` - uses batch processor
    - `test_updates_progress()` - job progress updated
    - `test_sets_job_completed()` - status on success
    - `test_sets_job_failed()` - status on fatal error
    - `test_records_errors()` - errors saved to job
    - `test_returns_job_id()` - immediate return for async
  - **Files**: `backend/services/import_service.py`, `backend/services/__init__.py`
  - **Implementation**:
    ```python
    class ImportService:
        def __init__(
            self,
            session: Session,
            job_repository: ImportJobRepository,
            qa_service: QAService,
            embedding_service: EmbeddingService,
        ):
            ...

        def start_import(
            self,
            filename: str,
            content: bytes,
            source_type: str,
        ) -> UUID:
            """Start import job, returns job ID immediately."""
            job = self._create_job(filename, source_type)
            # Queue background task
            return job.id

        async def process_import(self, job_id: UUID) -> ImportJob:
            """Process import (called by background task)."""
            # Parse → Validate → Batch process → Generate embeddings
    ```

### Task 62.4: Create Embedding Batch Trigger

- [ ] Trigger embedding generation for imported pairs (1 file, ~50 LOC)
  - **Tests**: `tests/unit/test_embedding_trigger.py`
    - `test_queues_embeddings_for_created_pairs()` - all pairs queued
    - `test_batches_embedding_calls()` - efficient API usage
    - `test_updates_job_on_embedding_complete()` - progress tracked
    - `test_handles_embedding_failure()` - continues with others
  - **Files**: `backend/services/embedding_trigger.py`
  - **Implementation**:
    ```python
    class EmbeddingTrigger:
        def __init__(self, embedding_service: EmbeddingService):
            self.embedding_service = embedding_service

        async def trigger_for_pairs(
            self,
            pair_ids: List[UUID],
            on_progress: Callable[[int, int], None] = None
        ) -> int:
            """Generate embeddings for list of QA pair IDs."""
    ```

### Task 62.5: Integrate with Background Tasks

- [ ] Wire import service to FastAPI background tasks (1 file, ~40 LOC)
  - **Tests**: `tests/integration/test_import_background.py`
    - `test_import_runs_in_background()` - non-blocking
    - `test_job_status_updates_during_processing()` - observable
    - `test_job_completes_after_processing()` - final status
    - `test_import_handles_crash_recovery()` - stale jobs detected
  - **Files**: `backend/tasks/import_tasks.py`
  - **Implementation**:
    ```python
    async def process_import_task(job_id: UUID):
        """Background task to process import job."""
        async with get_session() as session:
            service = ImportService(session, ...)
            await service.process_import(job_id)

    def schedule_import(background_tasks: BackgroundTasks, job_id: UUID):
        """Schedule import for background processing."""
        background_tasks.add_task(process_import_task, job_id)
    ```

---

## 3. Success Criteria

- [ ] Batch processor commits per batch for partial success
- [ ] Progress callbacks fire after each batch
- [ ] Single row errors don't abort entire import
- [ ] Import service orchestrates full flow
- [ ] Job status reflects current processing state
- [ ] Errors recorded with row numbers and messages
- [ ] Embeddings generated for imported pairs
- [ ] Background task integration non-blocking
- [ ] All tests pass: `pytest tests/unit/test_batch*.py tests/unit/test_import_service.py tests/integration/test_import_background.py -v`
- [ ] Coverage ≥80% for import service modules
