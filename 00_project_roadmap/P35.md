# P35: Token Management

## 1. Objective

**What**: Enforce token limits to prevent excessive costs and API errors. Count tokens before sending, truncate context if needed.

**Why**: LLMs have context window limits. Exceeding limits causes API errors. More tokens = more cost. Context quality matters more than quantity.

**Scope**:
- Included: Token counter, context truncation, max output enforcement, limit warnings
- Excluded: Dynamic context selection (future RAG improvements)

**Dependencies**: P34 (Cost Tracking)

---

## 2. Implementation Tasks

### Task 35.1: Token Counter

- [ ] Create token counting service (1 file, ~50 LOC)
  - **Tests**: `tests/unit/test_token_counter.py`
    - `test_counts_tokens_accurately()`
    - `test_counts_empty_string()`
    - `test_counts_unicode_text()`
    - `test_counts_code_blocks()`
    - `test_uses_correct_encoding_for_model()`
  - **Files**: `backend/llm/tokens.py`
  - **Implementation**:
    ```python
    import tiktoken

    class TokenCounter:
        _encodings: Dict[str, tiktoken.Encoding] = {}

        @classmethod
        def count(cls, text: str, model: str = "gpt-4o-mini") -> int:
            encoding = cls._get_encoding(model)
            return len(encoding.encode(text))

        @classmethod
        def _get_encoding(cls, model: str) -> tiktoken.Encoding:
            if model not in cls._encodings:
                try:
                    cls._encodings[model] = tiktoken.encoding_for_model(model)
                except KeyError:
                    # Fallback for unknown models
                    cls._encodings[model] = tiktoken.get_encoding("cl100k_base")
            return cls._encodings[model]

        @classmethod
        def count_messages(cls, messages: List[Dict], model: str) -> int:
            """Count tokens for chat messages format."""
            total = 0
            for msg in messages:
                total += 4  # Message overhead
                total += cls.count(msg.get("content", ""), model)
            total += 2  # Reply priming
            return total
    ```

### Task 35.2: Context Truncation

- [ ] Create context truncation logic (1 file, ~60 LOC)
  - **Tests**: `tests/unit/test_context_truncation.py`
    - `test_no_truncation_when_under_limit()`
    - `test_truncates_oldest_history_first()`
    - `test_preserves_system_prompt()`
    - `test_preserves_current_message()`
    - `test_preserves_retrieved_context()`
    - `test_warns_when_truncating()`
  - **Files**: `backend/llm/truncation.py`
  - **Implementation**:
    ```python
    @dataclass
    class TruncationConfig:
        max_context_tokens: int = 4000
        max_output_tokens: int = 1024
        reserve_for_output: int = 1024

    class ContextTruncator:
        def __init__(self, config: TruncationConfig):
            self.config = config
            self.counter = TokenCounter()

        def truncate(
            self,
            system_prompt: str,
            user_message: str,
            context: Optional[str],
            history: List[Dict],
            model: str,
        ) -> Tuple[str, List[Dict], bool]:
            """Truncate context to fit within limits.

            Returns (context, truncated_history, was_truncated)
            """
            available = self.config.max_context_tokens - self.config.reserve_for_output

            # Fixed costs: system prompt + current message
            fixed_tokens = (
                self.counter.count(system_prompt, model) +
                self.counter.count(user_message, model)
            )

            # Context is high priority
            context_tokens = self.counter.count(context or "", model)

            remaining = available - fixed_tokens - context_tokens
            if remaining < 0:
                # Even context is too big, truncate it
                context = self._truncate_text(context, available - fixed_tokens, model)
                return context, [], True

            # Truncate history from oldest
            truncated_history = []
            was_truncated = False
            for msg in reversed(history):
                msg_tokens = self.counter.count(msg.get("content", ""), model) + 4
                if remaining >= msg_tokens:
                    truncated_history.insert(0, msg)
                    remaining -= msg_tokens
                else:
                    was_truncated = True
                    break

            return context, truncated_history, was_truncated
    ```

### Task 35.3: Token Limit Enforcement

- [ ] Enforce limits in LLM service (1 file modification, ~40 LOC)
  - **Tests**: `tests/unit/test_cached_llm_service.py`
    - `test_truncates_context_when_needed()`
    - `test_respects_max_output_tokens()`
    - `test_logs_truncation_warning()`
  - **Files**: `backend/llm/service.py`
  - **Implementation**:
    ```python
    class LLMService:
        def __init__(
            self,
            ...,
            truncator: ContextTruncator,
        ):
            ...
            self.truncator = truncator

        async def generate(
            self,
            system_prompt: str,
            user_message: str,
            context: Optional[str] = None,
            history: Optional[List[Dict]] = None,
            max_tokens: int = 1024,
            **kwargs,
        ) -> LLMResponse:
            # Enforce token limits
            context, history, truncated = self.truncator.truncate(
                system_prompt, user_message, context, history or [], kwargs.get("model", "gpt-4o-mini")
            )
            if truncated:
                logger.warning("Context truncated to fit token limits")

            # Enforce max output tokens
            max_tokens = min(max_tokens, self.truncator.config.max_output_tokens)

            return await self._generate_with_cache(
                system_prompt, user_message, context, max_tokens=max_tokens, **kwargs
            )
    ```

### Task 35.4: Token Budget Reporting

- [ ] Add token usage reporting (1 file, ~30 LOC)
  - **Tests**: `tests/unit/test_token_counter.py`
    - `test_budget_report_includes_all_components()`
    - `test_budget_report_shows_remaining()`
  - **Files**: `backend/llm/tokens.py`
  - **Implementation**:
    ```python
    @dataclass
    class TokenBudget:
        system_prompt: int
        user_message: int
        context: int
        history: int
        total_input: int
        max_output: int
        remaining: int

    class TokenCounter:
        @classmethod
        def get_budget(
            cls,
            system_prompt: str,
            user_message: str,
            context: str,
            history: List[Dict],
            max_context: int,
            model: str,
        ) -> TokenBudget:
            sys_tokens = cls.count(system_prompt, model)
            user_tokens = cls.count(user_message, model)
            ctx_tokens = cls.count(context, model)
            hist_tokens = cls.count_messages(history, model)
            total = sys_tokens + user_tokens + ctx_tokens + hist_tokens
            return TokenBudget(
                system_prompt=sys_tokens,
                user_message=user_tokens,
                context=ctx_tokens,
                history=hist_tokens,
                total_input=total,
                max_output=1024,
                remaining=max_context - total,
            )
    ```

---

## 3. Success Criteria

- [ ] Token counting accurate for GPT models (tiktoken)
- [ ] Context truncated when exceeding limits
- [ ] Oldest history truncated first
- [ ] System prompt and current message always preserved
- [ ] Max output tokens enforced
- [ ] Truncation logged as warning
- [ ] Token budget reporting available
- [ ] All tests pass: `pytest tests/unit/test_token_counter.py tests/unit/test_context_truncation.py -v`
