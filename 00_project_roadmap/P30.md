# P30: OpenAI Provider

## 1. Objective

**What**: Implement the primary LLM provider using OpenAI's API. Handles text generation (GPT-4o-mini) and embeddings (text-embedding-3-small).

**Why**: GPT-4o-mini offers excellent quality-to-cost ratio. Reliable API with good uptime. This is the primary provider for production use.

**Scope**:
- Included: OpenAI provider class, rate limit handling, token counting, response parsing
- Excluded: Fallback logic (P32), caching (P33), cost tracking (P34)

**Dependencies**: P29 (Provider Abstraction)

---

## 2. Implementation Tasks

### Task 30.1: OpenAI Provider Implementation

- [ ] Implement OpenAI provider class (1 file, ~120 LOC)
  - **Tests**: `tests/unit/test_openai_provider.py`
    - `test_generate_returns_llm_response()`
    - `test_generate_uses_correct_model()`
    - `test_generate_includes_system_prompt()`
    - `test_generate_includes_context_when_provided()`
    - `test_generate_respects_max_tokens()`
    - `test_generate_respects_temperature()`
    - `test_embed_returns_vectors()`
    - `test_embed_handles_batch()`
    - `test_supports_embeddings_returns_true()`
  - **Files**: `backend/llm/providers/openai.py`
  - **Implementation**:
    ```python
    from openai import AsyncOpenAI
    from backend.llm.base import LLMProvider, LLMResponse, EmbeddingResponse
    from backend.llm.config import OpenAIConfig

    class OpenAIProvider(LLMProvider):
        def __init__(self, config: OpenAIConfig):
            self.config = config
            self.client = AsyncOpenAI(api_key=config.api_key)

        async def generate(
            self,
            system_prompt: str,
            user_message: str,
            context: Optional[str] = None,
            max_tokens: int = 1024,
            temperature: float = 0.7,
        ) -> LLMResponse:
            messages = [{"role": "system", "content": system_prompt}]
            if context:
                messages.append({"role": "user", "content": f"Context:\n{context}"})
            messages.append({"role": "user", "content": user_message})

            response = await self.client.chat.completions.create(
                model=self.config.model,
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
            )
            return LLMResponse(
                content=response.choices[0].message.content,
                model=response.model,
                input_tokens=response.usage.prompt_tokens,
                output_tokens=response.usage.completion_tokens,
                provider="openai",
            )

        async def embed(self, texts: List[str]) -> EmbeddingResponse:
            response = await self.client.embeddings.create(
                model=self.config.embedding_model,
                input=texts,
            )
            return EmbeddingResponse(
                vectors=[e.embedding for e in response.data],
                model=response.model,
                input_tokens=response.usage.total_tokens,
                provider="openai",
            )

        @property
        def supports_embeddings(self) -> bool:
            return True
    ```

### Task 30.2: Rate Limit Handling

- [ ] Add exponential backoff for rate limits (1 file modification, ~40 LOC)
  - **Tests**: `tests/unit/test_openai_provider.py`
    - `test_retries_on_rate_limit()`
    - `test_respects_retry_after_header()`
    - `test_raises_after_max_retries()`
    - `test_exponential_backoff_increases_delay()`
  - **Files**: `backend/llm/providers/openai.py`
  - **Implementation**:
    ```python
    async def _with_retry(self, operation: Callable) -> Any:
        last_error = None
        for attempt in range(self.config.max_retries):
            try:
                return await operation()
            except openai.RateLimitError as e:
                last_error = e
                retry_after = float(e.response.headers.get("retry-after", 0))
                delay = max(retry_after, (2 ** attempt) + random.uniform(0, 1))
                await asyncio.sleep(delay)
        raise RateLimitError(retry_after=delay)
    ```

### Task 30.3: Error Mapping

- [ ] Map OpenAI errors to provider errors (1 file modification, ~30 LOC)
  - **Tests**: `tests/unit/test_openai_provider.py`
    - `test_maps_auth_error()`
    - `test_maps_timeout_error()`
    - `test_maps_api_error()`
  - **Files**: `backend/llm/providers/openai.py`
  - **Implementation**:
    ```python
    def _map_error(self, error: Exception) -> LLMError:
        if isinstance(error, openai.AuthenticationError):
            return AuthenticationError("Invalid OpenAI API key")
        if isinstance(error, openai.APITimeoutError):
            return TimeoutError(self.config.timeout)
        if isinstance(error, openai.RateLimitError):
            return RateLimitError()
        return LLMError(str(error))
    ```

### Task 30.4: Register Provider

- [ ] Register OpenAI with provider registry (1 file, ~10 LOC)
  - **Tests**: `tests/unit/test_provider_registry.py`
    - `test_openai_registered_by_default()`
  - **Files**: `backend/llm/__init__.py`
  - **Implementation**:
    ```python
    from backend.llm.registry import ProviderRegistry
    from backend.llm.providers.openai import OpenAIProvider

    ProviderRegistry.register("openai", OpenAIProvider)
    ```

---

## 3. Success Criteria

- [ ] OpenAI provider implements LLMProvider interface
- [ ] Generate returns properly formatted LLMResponse
- [ ] Embed returns vectors with correct dimensions (1536)
- [ ] Rate limits handled with exponential backoff
- [ ] OpenAI errors mapped to provider error types
- [ ] Provider registered in registry
- [ ] All tests pass with mocked OpenAI client
