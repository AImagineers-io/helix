# P68: Improvement Plan for P61.md

## Target Phase

P61: Validation & Duplicate Detection

---

## What's Weak / Missing

- **O(n^2) duplicate detection**: In-file duplicate detection compares every pair; 1000 rows = 500k comparisons
- **Hardcoded similarity threshold**: 0.9 threshold has no empirical justification; could miss valid duplicates or flag false positives
- **String similarity only**: SequenceMatcher catches typos but misses semantic duplicates ("How do I return an item?" vs "What's the return process?")
- **DB duplicate detection inefficient**: Queries all active QA pairs into memory; 10k KB + 1k import = OOM risk
- **No content sanitization before validation**: HTML tags, excessive whitespace, control characters not cleaned
- **Missing XSS/injection checks**: Malicious content could be stored and rendered in admin UI
- **Answer-only duplicate detection missing**: Two different questions with identical answers may indicate copy-paste error
- **Short answer threshold arbitrary**: 20-char warning has no data backing; could be noise
- **No configurable validation rules**: All tenants get same rules; some may want stricter/looser limits
- **No fuzzy matching for categories**: "returns" vs "Returns" vs "RETURNS" treated as different categories

---

## Why This Matters

- **Import timeout risk**: O(n^2) comparison on 5000-row file takes minutes, causing timeouts or background task death
- **Knowledge base pollution**: String-only similarity misses semantic duplicates, leading to inconsistent bot answers
- **Security vulnerability**: Unsanitized content enables stored XSS when admin views QA pairs
- **Memory pressure**: Loading entire KB for duplicate check on large databases crashes import service
- **False positive warnings**: Too many low-value warnings train users to ignore all warnings

---

## Proposed Improvements

### 1. Optimize In-File Duplicate Detection

Replace O(n^2) with MinHash/LSH for approximate matching:

```python
from datasketch import MinHash, MinHashLSH

class InFileDuplicateDetector:
    def __init__(self, threshold: float = 0.9, num_perm: int = 128):
        self.threshold = threshold
        self.lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)
        self.minhashes = {}

    def find_duplicates(self, items: List[ParsedQA]) -> List[DuplicateMatch]:
        duplicates = []
        for i, item in enumerate(items):
            mh = self._create_minhash(item.question)

            # Query LSH for similar items
            candidates = self.lsh.query(mh)
            for candidate_idx in candidates:
                if candidate_idx < i:  # Only compare with earlier items
                    similarity = self._jaccard(mh, self.minhashes[candidate_idx])
                    if similarity >= self.threshold:
                        duplicates.append(DuplicateMatch(
                            source_line=item.source_line,
                            matched_line=items[candidate_idx].source_line,
                            similarity=similarity
                        ))

            self.lsh.insert(i, mh)
            self.minhashes[i] = mh

        return duplicates

    def _create_minhash(self, text: str) -> MinHash:
        mh = MinHash(num_perm=128)
        for word in text.lower().split():
            mh.update(word.encode('utf-8'))
        return mh
```

Complexity: O(n) average case with LSH.

Add tests:
- `test_handles_5000_items_under_10_seconds()`
- `test_lsh_detects_near_duplicates()`

### 2. Add Embedding-Based Semantic Duplicate Detection

```python
class SemanticDuplicateDetector:
    """Use existing embeddings for semantic similarity."""

    def __init__(self, embedding_service: EmbeddingService, threshold: float = 0.85):
        self.embedding_service = embedding_service
        self.threshold = threshold

    async def find_semantic_duplicates(
        self,
        items: List[ParsedQA],
        existing_embeddings: List[Tuple[UUID, List[float]]]
    ) -> List[DBDuplicateMatch]:
        # Batch embed new questions
        new_embeddings = await self.embedding_service.embed_batch(
            [item.question for item in items]
        )

        # Use FAISS for efficient similarity search
        import faiss
        index = faiss.IndexFlatIP(1536)  # Inner product for cosine sim
        existing_matrix = np.array([e[1] for e in existing_embeddings])
        faiss.normalize_L2(existing_matrix)
        index.add(existing_matrix)

        duplicates = []
        for i, (item, embedding) in enumerate(zip(items, new_embeddings)):
            query = np.array([embedding])
            faiss.normalize_L2(query)
            D, I = index.search(query, k=5)  # Top 5 matches

            for distance, idx in zip(D[0], I[0]):
                if distance >= self.threshold:
                    duplicates.append(DBDuplicateMatch(
                        source_line=item.source_line,
                        existing_id=existing_embeddings[idx][0],
                        existing_question="",  # Fetch if needed
                        similarity=float(distance)
                    ))

        return duplicates
```

Add tests:
- `test_detects_semantic_duplicate()`
- `test_ignores_semantically_different()`

### 3. Implement Efficient DB Duplicate Check

```python
class DBDuplicateDetector:
    def find_duplicates(self, items: List[ParsedQA]) -> List[DBDuplicateMatch]:
        # Use trigram similarity in PostgreSQL instead of loading all
        duplicates = []
        questions = [item.question for item in items]

        # Batch query with pg_trgm
        query = """
            SELECT import_q, qa.id, qa.question, similarity(import_q, qa.question) as sim
            FROM unnest(:questions) AS import_q
            JOIN qa_pair qa ON similarity(import_q, qa.question) > :threshold
            WHERE qa.status = 'active' AND qa.deleted_at IS NULL
            ORDER BY sim DESC
        """

        results = self.session.execute(query, {
            "questions": questions,
            "threshold": self.threshold
        })

        # Map back to source lines
        question_to_line = {item.question: item.source_line for item in items}
        for row in results:
            duplicates.append(DBDuplicateMatch(
                source_line=question_to_line[row.import_q],
                existing_id=row.id,
                existing_question=row.question,
                similarity=row.sim
            ))

        return duplicates
```

Requires migration to enable pg_trgm extension.

### 4. Add Content Sanitization

```python
import bleach
import re

class ContentSanitizer:
    def sanitize(self, text: str) -> str:
        # Strip HTML tags
        text = bleach.clean(text, tags=[], strip=True)

        # Normalize whitespace
        text = re.sub(r'\s+', ' ', text)

        # Remove control characters except newlines
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)

        # Strip leading/trailing whitespace
        return text.strip()

    def check_xss(self, text: str) -> List[str]:
        """Return list of XSS indicators found."""
        patterns = [
            (r'<script', 'script tag'),
            (r'javascript:', 'javascript protocol'),
            (r'on\w+\s*=', 'event handler'),
            (r'data:', 'data URI'),
        ]
        findings = []
        for pattern, name in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                findings.append(name)
        return findings
```

Add tests:
- `test_strips_html_tags()`
- `test_normalizes_whitespace()`
- `test_detects_xss_script_tag()`
- `test_detects_xss_event_handler()`

### 5. Add Configurable Validation Rules

```python
@dataclass
class ValidationConfig:
    max_question_length: int = 10000
    max_answer_length: int = 50000
    min_answer_length_warning: int = 20
    require_category: bool = False
    max_tags: int = 10
    allowed_categories: Optional[Set[str]] = None  # None = any

    @classmethod
    def from_env(cls) -> "ValidationConfig":
        return cls(
            max_question_length=int(os.getenv("IMPORT_MAX_QUESTION_LENGTH", 10000)),
            # ...
        )

class QAValidator:
    def __init__(self, config: ValidationConfig = None):
        self.config = config or ValidationConfig.from_env()
```

### 6. Normalize Categories

```python
def normalize_category(self, category: str) -> str:
    """Normalize category for consistent matching."""
    if not category:
        return None

    # Lowercase, strip whitespace
    normalized = category.lower().strip()

    # Common variations mapping
    CATEGORY_ALIASES = {
        "return": "returns",
        "refund": "returns",
        "shipping": "delivery",
        "ship": "delivery",
    }

    return CATEGORY_ALIASES.get(normalized, normalized)
```

### 7. Add Answer Similarity Check

```python
def find_identical_answers(self, items: List[ParsedQA]) -> List[dict]:
    """Find items with identical answers but different questions."""
    answer_hash = {}
    warnings = []

    for item in items:
        answer_key = hashlib.md5(item.answer.lower().encode()).hexdigest()
        if answer_key in answer_hash:
            warnings.append({
                "type": "identical_answer",
                "lines": [answer_hash[answer_key], item.source_line],
                "message": "Different questions have identical answers"
            })
        else:
            answer_hash[answer_key] = item.source_line

    return warnings
```

---

## Impact on Roadmap

| Affected Phase | Change |
|----------------|--------|
| P59 | Add migration for pg_trgm extension |
| P60 | Parsers must call sanitizer on parsed content |
| P62 | Import service passes config to validator |
| P63 | Add validation config to import request (optional) |
| P65 | Add performance tests for 5000-row duplicate detection |

---

## New Tasks for P61

- **Task 61.6**: Implement MinHash/LSH for O(n) in-file duplicate detection
- **Task 61.7**: Add pg_trgm-based DB duplicate detection
- **Task 61.8**: (Optional) Add embedding-based semantic duplicate detection
- **Task 61.9**: Create ContentSanitizer with HTML stripping and XSS detection
- **Task 61.10**: Implement ValidationConfig for configurable rules
- **Task 61.11**: Add category normalization with alias mapping
- **Task 61.12**: Add identical answer detection as warning

---

## Dependency Changes

Add to `backend/requirements.txt`:
```
datasketch>=1.6.0  # MinHash/LSH
bleach>=6.0.0      # HTML sanitization
faiss-cpu>=1.7.4   # Optional: for semantic similarity
```

Add migration:
```python
# Enable pg_trgm extension for fuzzy text matching
op.execute("CREATE EXTENSION IF NOT EXISTS pg_trgm")
op.execute("CREATE INDEX ix_qa_pair_question_trgm ON qa_pair USING gin (question gin_trgm_ops)")
```
