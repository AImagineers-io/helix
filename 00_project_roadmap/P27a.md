# P27a: Chat Pipeline - Architecture & Context

## 1. Objective

**What**: Build the chat pipeline framework with processor chaining, context object, and graceful error handling.

**Why**: The pipeline orchestrates all message processing. A clean architecture enables:
- Single-responsibility processors that are independently testable
- Configuration-driven enabling/disabling of processors
- Natural logging/observability points between steps
- Graceful degradation when optional processors fail

**Scope**:
- Included: Pipeline orchestrator, processor base class, pipeline context object, processor registration
- Excluded: Individual processors (P27b-P27d), API integration (P28)

**Dependencies**: P15b (QA Service), P13 (pgvector for RAG)

---

## 2. Implementation Tasks

### Task 27a.1: Create Processor Base Class

- [ ] Define abstract processor interface (2 files, ~60 LOC)
  - **Tests**: `tests/unit/test_pipeline_processor.py`
    - `test_processor_has_name()` - name property required
    - `test_processor_has_process_method()` - abstract method enforced
    - `test_processor_can_be_optional()` - is_optional flag
    - `test_processor_emits_timing()` - duration tracked
  - **Files**: `backend/pipeline/base.py`, `backend/pipeline/__init__.py`
  - **Implementation**:
    ```python
    class Processor(ABC):
        name: str
        is_optional: bool = False

        @abstractmethod
        async def process(self, context: PipelineContext) -> PipelineContext:
            """Process context and return modified context."""

        def should_skip(self, context: PipelineContext) -> bool:
            """Override to conditionally skip processor."""
            return False
    ```

### Task 27a.2: Create Pipeline Context Object

- [ ] Build immutable context that accumulates data through pipeline (1 file, ~100 LOC)
  - **Tests**: `tests/unit/test_pipeline_context.py`
    - `test_context_stores_original_message()` - immutable original
    - `test_context_stores_detected_language()` - language detection result
    - `test_context_stores_translated_text()` - translation result
    - `test_context_stores_retrieved_qa()` - RAG results
    - `test_context_stores_generated_response()` - LLM response
    - `test_context_tracks_processor_timings()` - timing dict
    - `test_context_tracks_conversation_history()` - multi-turn
    - `test_context_is_immutable()` - returns new instance on update
    - `test_context_serializes_to_dict()` - for logging/debugging
  - **Files**: `backend/pipeline/context.py`
  - **Implementation**:
    ```python
    @dataclass(frozen=True)
    class PipelineContext:
        # Input
        original_message: str
        device_id: str
        conversation_id: Optional[UUID] = None
        conversation_history: Tuple[Message, ...] = ()

        # Processing state
        detected_language: Optional[str] = None
        translated_message: Optional[str] = None
        is_moderated: bool = False
        moderation_reason: Optional[str] = None
        intent: Optional[str] = None
        retrieved_qa_pairs: Tuple[QAPairResult, ...] = ()
        generated_response: Optional[str] = None
        final_response: Optional[str] = None

        # Metadata
        processor_timings: Tuple[ProcessorTiming, ...] = ()
        errors: Tuple[ProcessorError, ...] = ()

        def with_update(self, **kwargs) -> "PipelineContext":
            """Return new context with updated fields."""
    ```

### Task 27a.3: Create Pipeline Orchestrator

- [ ] Build orchestrator that chains processors (1 file, ~80 LOC)
  - **Tests**: `tests/unit/test_pipeline_orchestrator.py`
    - `test_orchestrator_runs_processors_in_order()` - sequence preserved
    - `test_orchestrator_passes_context_through_chain()` - data flows
    - `test_orchestrator_skips_disabled_processors()` - config respected
    - `test_orchestrator_continues_on_optional_failure()` - graceful degradation
    - `test_orchestrator_stops_on_required_failure()` - critical errors halt
    - `test_orchestrator_collects_timings()` - all processors timed
    - `test_orchestrator_logs_errors()` - errors captured in context
  - **Files**: `backend/pipeline/orchestrator.py`
  - **Implementation**:
    ```python
    class PipelineOrchestrator:
        def __init__(self, processors: List[Processor], config: PipelineConfig):
            self._processors = processors
            self._config = config

        async def process(self, context: PipelineContext) -> PipelineContext:
            """Run message through all enabled processors."""

        def _is_processor_enabled(self, processor: Processor) -> bool:
            """Check config if processor should run."""
    ```

### Task 27a.4: Create Pipeline Configuration

- [ ] Environment-driven pipeline config (1 file, ~50 LOC)
  - **Tests**: `tests/unit/test_pipeline_config.py`
    - `test_config_loads_from_env()` - reads environment variables
    - `test_config_has_processor_toggles()` - enable/disable each
    - `test_config_has_defaults()` - sensible defaults
    - `test_config_validates_values()` - rejects invalid config
  - **Files**: `backend/pipeline/config.py`
  - **Implementation**:
    ```python
    @dataclass
    class PipelineConfig:
        language_detection_enabled: bool = True
        translation_enabled: bool = True
        moderation_enabled: bool = True
        intent_classification_enabled: bool = True
        rag_retrieval_enabled: bool = True
        rag_top_k: int = 3
        rag_min_similarity: float = 0.7

        @classmethod
        def from_env(cls) -> "PipelineConfig":
            """Load config from environment variables."""
    ```

### Task 27a.5: Create Pipeline Factory

- [ ] Factory to assemble pipeline with DI (1 file, ~40 LOC)
  - **Tests**: `tests/unit/test_pipeline_factory.py`
    - `test_factory_creates_full_pipeline()` - all processors
    - `test_factory_injects_dependencies()` - services injected
    - `test_factory_respects_config()` - disabled processors excluded
  - **Files**: `backend/pipeline/factory.py`
  - **Implementation**:
    ```python
    class PipelineFactory:
        def __init__(self, session: Session, config: PipelineConfig):
            self._session = session
            self._config = config

        def create(self) -> PipelineOrchestrator:
            """Create pipeline with all enabled processors."""
    ```

---

## 3. Success Criteria

- [ ] Processor base class enforces contract (name, process method)
- [ ] Context is immutable and accumulates data correctly
- [ ] Orchestrator runs processors in order
- [ ] Optional processor failures don't halt pipeline
- [ ] Required processor failures stop processing
- [ ] All processors are timed and timings stored in context
- [ ] Config enables/disables processors via environment
- [ ] Factory assembles pipeline with proper dependency injection
- [ ] All tests pass: `pytest tests/unit/test_pipeline_*.py -v`
- [ ] Coverage â‰¥80% for `backend/pipeline/`
